{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b4b360-cc1b-4298-9d96-5eb939dcdecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from medpy.metric import dc, assd\n",
    "import torch.nn.functional as F\n",
    "import torchio as tio\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b0b30b-d1e0-486a-945f-94fae1c0b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     gpu = torch.cuda.current_device()\n",
    "#     gpu_properties = torch.cuda.get_device_properties(gpu)\n",
    "#     print(f\"GPU Name: {gpu_properties.name}\")\n",
    "#     print(f\"GPU Memory (in bytes): {gpu_properties.total_memory}\")\n",
    "#     print(f\"GPU Memory (in GB): {gpu_properties.total_memory / (1024 ** 3)}\")\n",
    "# else:\n",
    "#     print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7239878d-0d7b-4f6c-93fe-73301fad57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BrainSegmentationDataset(Dataset):\n",
    "    def __init__(self, t1c_dir, t2f_dir, seg_dir, transforms=None):\n",
    "        self.t1c_dir = t1c_dir\n",
    "        self.t2f_dir = t2f_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Create a list of all seg images (patient + slice)\n",
    "        self.seg_files = [os.path.join(root, file) for root, _, files in os.walk(seg_dir) \n",
    "                          for file in files if os.path.getsize(os.path.join(root, file)) > 141]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seg_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seg_img_path = self.seg_files[idx]\n",
    "        \n",
    "        mask = Image.open(seg_img_path).convert('L')\n",
    "        \n",
    "        mask = np.array(mask, dtype = np.int64)\n",
    "        \n",
    "        background_mask = (mask == 0).astype(np.float32)\n",
    "        class1_mask = (mask == 85).astype(np.float32)\n",
    "        class2_mask = (mask == 170).astype(np.float32)\n",
    "        class3_mask = (mask == 255).astype(np.float32)\n",
    "        \n",
    "        mask = np.dstack([background_mask, class1_mask, class2_mask, class3_mask])\n",
    "        mask = np.argmax(mask, axis = 2)\n",
    "        \n",
    "        # Corresponding input images\n",
    "        t1c_img_path = seg_img_path.replace('seg', 't1n')\n",
    "        t1c_img = Image.open(t1c_img_path).convert('L')  # load and convert t1c image\n",
    "        t1c_tensor = torch.from_numpy(np.array(t1c_img,dtype = np.float32)[None, ...])/255.0\n",
    "#         t1c_tensor = self.transforms(t1c_img)  # transform to tensor\n",
    "\n",
    "        t2f_img_path = seg_img_path.replace('seg', 't2f')\n",
    "        t2f_img = Image.open(t2f_img_path)  # load and convert t2f image\n",
    "        t2f_tensor = torch.from_numpy(np.array(t2f_img,dtype = np.float32)[None, ...])/255.0\n",
    "#         t2f_tensor = self.transforms(t2f_img)  # transform to tensor\n",
    "\n",
    "        # Stack t1c and t2f tensors along the channel dimension\n",
    "        inputs = torch.cat((t1c_tensor, t2f_tensor), dim=0)\n",
    "\n",
    "        return inputs, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843d14cc-a62f-4615-80b2-8a1fa1ef2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "     # You can modify this as per your model's input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = BrainSegmentationDataset(t1c_dir=\"BraTS_2023_small/t1nSliced\", t2f_dir=\"BraTS_2023_small/t2fSliced\", seg_dir=\"BraTS_2023_small/segSliced\", transforms=data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2f4a74-0d58-4986-9cef-33c18ef12cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split dataset into training set and validation set\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create a DataLoader for both sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6c1703-1df0-4d30-a1e5-718984ff5caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17713\n",
      "2215\n",
      "Data shape: torch.Size([8, 2, 256, 256])\n",
      "Labels shape: torch.Size([8, 256, 256])\n",
      "Data shape: torch.Size([8, 2, 256, 256])\n",
      "Labels shape: torch.Size([8, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "data, labels = next(iter(train_loader))\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "data, labels = next(iter(val_loader))\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3ed452-aae6-4638-988f-8f8e4ad4f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_and_masks(preds, masks):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "    for i, (pred, mask) in enumerate(zip(preds, masks)):\n",
    "#         pred = torch.argmax(pred, dim=0)  # Convert from one-hot encoding to class labels\n",
    "#         mask = torch.argmax(mask, dim=0)  # Convert from one-hot encoding to class labels\n",
    "\n",
    "        axs[0, i].imshow(pred.cpu().detach().numpy(), cmap = 'gray')\n",
    "        axs[0, i].title.set_text(\"Predicted\")\n",
    "        axs[1, i].imshow(mask.cpu().detach().numpy(), cmap = 'gray')\n",
    "        axs[1, i].title.set_text(\"Ground Truth\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9faeabbc-6929-4c11-8dcc-0b9bcfa3e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, target):\n",
    "    assert pred.shape == target.shape\n",
    "    batch_size, num_classes, _, _ = pred.shape\n",
    "    DSCs = []\n",
    "    ASSDs = []\n",
    "    for image_idx in range(batch_size):\n",
    "        dsc_per_class = []\n",
    "        assd_per_class = []\n",
    "        for class_idx in range(num_classes):\n",
    "            pred_class = pred[image_idx, class_idx]\n",
    "            target_class = target[image_idx, class_idx]\n",
    "\n",
    "            # Check if the prediction and the target contain any objects\n",
    "            pred_has_object = np.count_nonzero(pred_class) > 0\n",
    "            target_has_object = np.count_nonzero(target_class) > 0\n",
    "            \n",
    "            if pred_has_object and target_has_object:\n",
    "                dsc = dc(pred_class, target_class)\n",
    "                assd_val = assd(pred_class, target_class)\n",
    "            elif not pred_has_object and not target_has_object:\n",
    "                dsc = 1.0  # perfect similarity\n",
    "                assd_val = 0.0  # no distance\n",
    "            else:\n",
    "                dsc = 0.0  # no overlap\n",
    "                assd_val = 362  # maximum distance\n",
    "\n",
    "            dsc_per_class.append(dsc)\n",
    "            assd_per_class.append(assd_val)\n",
    "        \n",
    "        DSCs.append(dsc_per_class)\n",
    "        ASSDs.append(assd_per_class)\n",
    "    DSCs = np.array(DSCs)\n",
    "    ASSDs = np.array(ASSDs)\n",
    "        \n",
    "    return np.mean(DSCs, axis = 0), np.mean(ASSDs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4333d19-0944-4bf5-ba3e-a36dad399536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.psi(F.relu(g1 + x1, inplace=True))\n",
    "        return x * psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41094667-f509-41fb-8ea9-7e426785a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(UNet, self).__init__()\n",
    "\n",
    "#         # Downsample path\n",
    "#         self.conv1 = self.double_conv(in_channels, 64)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.conv2 = self.double_conv(64, 128)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.conv3 = self.double_conv(128, 256)\n",
    "#         self.pool3 = nn.MaxPool2d(2)\n",
    "#         self.conv4 = self.double_conv(256, 512)\n",
    "#         self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        \n",
    "#         self.attention_block1 = AttentionBlock(F_g=512, F_l=512, F_int=256)\n",
    "#         self.attention_block2 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "#         self.attention_block3 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "#         self.attention_block4 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "#         # Bottom\n",
    "#         self.conv5 = self.double_conv(512, 1024)\n",
    "        \n",
    "\n",
    "#         # Upsample path\n",
    "#         self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size = 2, stride = 2)\n",
    "#         self.conv6 = self.double_conv(1024, 512)\n",
    "#         self.up7 = nn.ConvTranspose2d(512, 256, kernel_size = 2, stride = 2)\n",
    "#         self.conv7 = self.double_conv(512, 256)\n",
    "#         self.up8 = nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = 2)\n",
    "#         self.conv8 = self.double_conv(256, 128)\n",
    "#         self.up9 = nn.ConvTranspose2d(128, 64, kernel_size = 2, stride = 2)\n",
    "#         self.conv9 = self.double_conv(128, 64)\n",
    "\n",
    "#         # Output\n",
    "#         self.conv10 = nn.Conv2d(64, out_channels, kernel_size = 1)\n",
    "\n",
    "#     def double_conv(self, in_channels, out_channels):\n",
    "#         return nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.BatchNorm2d(out_channels)\n",
    "            \n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Downsample path\n",
    "#         c1 = self.conv1(x)\n",
    "#         p1 = self.pool1(c1)\n",
    "#         c2 = self.conv2(p1)\n",
    "#         p2 = self.pool2(c2)\n",
    "#         c3 = self.conv3(p2)\n",
    "#         p3 = self.pool3(c3)\n",
    "#         c4 = self.conv4(p3)\n",
    "#         p4 = self.pool4(c4)\n",
    "\n",
    "#         # Bottom\n",
    "#         c5 = self.conv5(p4)\n",
    "\n",
    "#         # Upsample path\n",
    "#         up6 = self.up6(c5)\n",
    "#         # print(f\"up6 size: {up6.size()}, c4 size: {c4.size()}\")\n",
    "#         merge6 = torch.cat([up6, c4], dim = 1)\n",
    "#         c6 = self.conv6(merge6)\n",
    "#         up7 = self.up7(c6)\n",
    "#         # print(f\"up7 size: {up7.size()}, c3 size: {c3.size()}\")\n",
    "#         merge7 = torch.cat([up7, c3], dim = 1)\n",
    "#         c7 = self.conv7(merge7)\n",
    "#         up8 = self.up8(c7)\n",
    "#         # print(f\"up8 size: {up8.size()}, c2 size: {c2.size()}\")\n",
    "#         merge8 = torch.cat([up8, c2], dim = 1)\n",
    "#         c8 = self.conv8(merge8)\n",
    "#         up9 = self.up9(c8)\n",
    "#         # print(f\"up9 size: {up9.size()}, c1 size: {c1.size()}\")\n",
    "#         merge9 = torch.cat([up9, c1], dim = 1)\n",
    "#         c9 = self.conv9(merge9)\n",
    "\n",
    "#         # Output\n",
    "#         out = self.conv10(c9)\n",
    "#         return out\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         out = self.forward(x)\n",
    "#         _, preds = torch.max(out, 1)\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65bc16cd-4a22-4ad4-90ce-1680bb795bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AttentionBlock(nn.Module):\n",
    "#     def __init__(self, F_g, F_l, F_int):\n",
    "#         super(AttentionBlock, self).__init__()\n",
    "\n",
    "#         self.W_g = nn.Sequential(\n",
    "#             nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "#             nn.BatchNorm2d(F_int)\n",
    "#         )\n",
    "\n",
    "#         self.W_x = nn.Sequential(\n",
    "#             nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "#             nn.BatchNorm2d(F_int)\n",
    "#         )\n",
    "\n",
    "#         self.psi = nn.Sequential(\n",
    "#             nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "#             nn.BatchNorm2d(1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, g, x):\n",
    "#         g1 = self.W_g(g)\n",
    "#         x1 = self.W_x(x)\n",
    "#         psi = self.psi(F.relu(g1 + x1, inplace=True))\n",
    "#         return x * psi\n",
    "\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(UNet, self).__init__()\n",
    "\n",
    "#         # Downsample path\n",
    "#         self.conv1 = self.double_conv(in_channels, 64)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.conv2 = self.double_conv(64, 128)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.conv3 = self.double_conv(128, 256)\n",
    "#         self.pool3 = nn.MaxPool2d(2)\n",
    "#         self.conv4 = self.double_conv(256, 512)\n",
    "#         self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "#         # Bottom\n",
    "#         self.conv5 = self.double_conv(512, 1024)\n",
    "\n",
    "#         # Attention blocks\n",
    "#         self.attention_block1 = AttentionBlock(F_g=512, F_l=512, F_int=256)\n",
    "#         self.attention_block2 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "#         self.attention_block3 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "#         self.attention_block4 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "\n",
    "#         # Upsample path\n",
    "#         self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size = 2, stride = 2)\n",
    "#         self.conv6 = self.double_conv(1024, 512)\n",
    "#         self.up7 = nn.ConvTranspose2d(512, 256, kernel_size = 2, stride = 2)\n",
    "#         self.conv7 = self.double_conv(512, 256)\n",
    "#         self.up8 = nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = 2)\n",
    "#         self.conv8 = self.double_conv(256, 128)\n",
    "#         self.up9 = nn.ConvTranspose2d(128, 64, kernel_size = 2, stride = 2)\n",
    "#         self.conv9 = self.double_conv(128, 64)\n",
    "\n",
    "#         # Output\n",
    "#         self.conv10 = nn.Conv2d(64, out_channels, kernel_size = 1)\n",
    "\n",
    "#     def double_conv(self, in_channels, out_channels):\n",
    "#         return nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.BatchNorm2d(out_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Downsample path\n",
    "#         c1 = self.conv1(x)\n",
    "#         p1 = self.pool1(c1)\n",
    "#         c2 = self.conv2(p1)\n",
    "#         p2 = self.pool2(c2)\n",
    "#         c3 = self.conv3(p2)\n",
    "#         p3 = self.pool3(c3)\n",
    "#         c4 = self.conv4(p3)\n",
    "#         p4 = self.pool4(c4)\n",
    "\n",
    "#         # Bottom\n",
    "#         c5 = self.conv5(p4)\n",
    "\n",
    "#         # Upsample path\n",
    "#         up6 = self.up6(c5)\n",
    "#         attention_map1 = self.attention_block1(up6, c4)\n",
    "#         merge6 = torch.cat([up6, attention_map1], dim = 1)\n",
    "#         c6 = self.conv6(merge6)\n",
    "\n",
    "#         up7 = self.up7(c6)\n",
    "#         attention_map2 = self.attention_block2(up7, c3)\n",
    "#         merge7 = torch.cat([up7, attention_map2], dim = 1)\n",
    "#         c7 = self.conv7(merge7)\n",
    "\n",
    "#         up8 = self.up8(c7)\n",
    "#         attention_map3 = self.attention_block3(up8, c2)\n",
    "#         merge8 = torch.cat([up8, attention_map3], dim = 1)\n",
    "#         c8 = self.conv8(merge8)\n",
    "\n",
    "#         up9 = self.up9(c8)\n",
    "#         attention_map4 = self.attention_block4(up9, c1)\n",
    "#         merge9 = torch.cat([up9, attention_map4], dim = 1)\n",
    "#         c9 = self.conv9(merge9)\n",
    "\n",
    "#         # Output\n",
    "#         out = self.conv10(c9)\n",
    "#         return out\n",
    "\n",
    "#     def predict(self, x):\n",
    "#         out = self.forward(x)\n",
    "#         _, preds = torch.max(out, 1)\n",
    "#         return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0e9ff4-b8b9-424e-82bd-6cbad0913ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            double_conv(ch_in,ch_out)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdea888d-549b-4963-8047-bc49f8d9501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int, n_heads):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.F_int = F_int\n",
    "        self.W_g = nn.ModuleList([nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True) for _ in range(n_heads)])\n",
    "        self.W_x = nn.ModuleList([nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True) for _ in range(n_heads)])\n",
    "\n",
    "        self.psi = nn.ModuleList([nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True) for _ in range(n_heads)])\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        heads = []\n",
    "        for i in range(self.n_heads):\n",
    "            g1 = self.W_g[i](g)\n",
    "            x1 = self.W_x[i](x)\n",
    "            psi = self.relu(g1 + x1)\n",
    "            psi = self.psi[i](psi)\n",
    "            psi = self.sigmoid(psi)\n",
    "            heads.append(x * psi)\n",
    "\n",
    "        return sum(heads) / self.n_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba022d0d-0264-4573-aca7-cf03bc81cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUnet(nn.Module):\n",
    "    def __init__(self, in_channels = 2,out_channels = 4):\n",
    "        super(AttentionUnet, self).__init__()\n",
    "        \n",
    "\n",
    "        n1 = 64\n",
    "        n2 = n1 * 2\n",
    "        n3 = n1 * 4\n",
    "        n4 = n1 * 8\n",
    "        n5 = n1 * 16\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = double_conv(in_channels, n1)\n",
    "        self.Conv2 = double_conv(n1, n2)\n",
    "        self.Conv3 = double_conv(n2, n3)\n",
    "        self.Conv4 = double_conv(n3, n4)\n",
    "        self.Conv5 = double_conv(n4, n5)\n",
    "\n",
    "        self.Up5 = up_conv(n5, n4)\n",
    "        self.Att5 = MultiHeadAttentionBlock(F_g=n4, F_l=n4, F_int=n3, n_heads=2)\n",
    "        self.Up_conv5 = double_conv(n5, n4)\n",
    "\n",
    "        self.Up4 = up_conv(n4, n3)\n",
    "        self.Att4 = MultiHeadAttentionBlock(F_g=n3, F_l=n3, F_int=n2, n_heads=2)\n",
    "        self.Up_conv4 = double_conv(n4, n3)\n",
    "\n",
    "        self.Up3 = up_conv(n3, n2)\n",
    "        self.Att3 = MultiHeadAttentionBlock(F_g=n2, F_l=n2, F_int=n1, n_heads=2)\n",
    "        self.Up_conv3 = double_conv(n3, n2)\n",
    "\n",
    "        self.Up2 = up_conv(n2, n1)\n",
    "        self.Att2 = MultiHeadAttentionBlock(F_g=n1, F_l=n1, F_int=int(n1/2), n_heads=2)\n",
    "        self.Up_conv2 = double_conv(n2, n1)\n",
    "\n",
    "        self.Conv = nn.Conv2d(n1, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "871e6edc-ee1e-4f5a-ac83-9c923ecfd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-7):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        intersection = (output * target).sum(dim=(2, 3))\n",
    "        union = output.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "\n",
    "        dice_coeff = (2. * intersection + self.eps) / (union + self.eps)\n",
    "        dice_loss = 1. - dice_coeff\n",
    "\n",
    "        return dice_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9c6403-ee32-48cd-83a2-b49408da3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# def train_model(num_epochs):\n",
    "#     # Create dataset and dataloader for T1\n",
    "# #     training_dataset_T1 = train_loader\n",
    "# #     validation_dataset_T1 = val_loader\n",
    "\n",
    "#     training_dataloader_T1 = train_loader\n",
    "#     validation_dataloader_T1 = val_loader\n",
    "\n",
    "#     # Check if saved model exists, if so load it, otherwise initialize a new one\n",
    "#     model_path = \"unet_model_att.pth\"\n",
    "#     if os.path.isfile(model_path):\n",
    "#         print(\"Loading model...\")\n",
    "#         model = UNet(in_channels = 2, out_channels = 4)\n",
    "#         model.load_state_dict(torch.load(model_path))\n",
    "#         model = model.to(device)\n",
    "#         print(\"Model loaded.\")\n",
    "#     else:\n",
    "#         print(\"Initializing new model...\")\n",
    "#         model = UNet(in_channels = 2, out_channels = 4).to(device)\n",
    "#         print(\"New model initialized.\")\n",
    "\n",
    "#     # Define loss function and optimizer\n",
    "#     # criterion = nn.CrossEntropyLoss()\n",
    "#     criterion = DiceLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"Epoch { epoch + 1 }/{ num_epochs }\")\n",
    "#         print(\"-------------------------\")\n",
    "\n",
    "#         # Initialize the running loss to zero\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         # Initialize counter for the number of batches\n",
    "#         num_batches = 0\n",
    "\n",
    "#         # Initialize the running DSC and ASSD\n",
    "#         running_train_dsc = []\n",
    "#         running_train_assd = []\n",
    "\n",
    "#         # Training phase\n",
    "#         model.train() \n",
    "#         for i, (images, masks) in enumerate(tqdm(training_dataloader_T1)):\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.to(device)\n",
    "# #             masks = masks.float().squeeze(1).permute(0, 3, 2, 1)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(images)\n",
    "#             masks_one_hot = F.one_hot(masks, num_classes = 4).permute(0, 3, 1, 2).contiguous().float()\n",
    "            \n",
    "#             probs = F.softmax(outputs, dim=1)\n",
    "#             # loss = criterion(probs, masks)\n",
    "#             loss = criterion(probs, masks_one_hot)\n",
    "            \n",
    "#             # loss = criterion(outputs, masks)\n",
    "\n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if epoch % 1 == 0 and i == 0:\n",
    "#                 print(\"Training Prediction Visualization:\")\n",
    "#                 _, preds = torch.max(outputs[:2], 1)\n",
    "# #                 print(outputs.shape,masks.shape)\n",
    "#                 plot_preds_and_masks(preds,masks[:2])\n",
    "\n",
    "#             # Update the running loss and batch count\n",
    "#             running_loss += loss.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "\n",
    "#             if epoch % 1 == 0:\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 preds_one_hot = F.one_hot(preds, num_classes = 4)\n",
    "#                 preds_one_hot = preds_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "#                 masks_one_hot = F.one_hot(masks, num_classes = 4)\n",
    "#                 masks_one_hot = masks_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "#                 train_dsc, train_assd = compute_metrics(preds_one_hot.cpu().numpy(), masks_one_hot.cpu().numpy())\n",
    "#                 running_train_dsc.append(train_dsc)\n",
    "#                 running_train_assd.append(train_assd)\n",
    "\n",
    "\n",
    "#         # Compute and print the average training loss for this epoch\n",
    "#         avg_train_loss = running_loss / num_batches\n",
    "#         print(f\"\\nAverage Training Loss: {avg_train_loss}\")\n",
    "\n",
    "#         if epoch % 1 == 0:\n",
    "#             print(f\"\\nAverage Training DSC: {np.mean(np.array(running_train_dsc), axis = 0)}\\nAverage Training ASSD: {np.mean(np.array(running_train_assd), axis = 0)}\")\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()  # Set model to evaluate mode\n",
    "#         running_val_loss = 0.0\n",
    "#         num_val_batches = 0\n",
    "#         running_val_dsc = []\n",
    "#         running_val_assd = []\n",
    "\n",
    "#         with torch.no_grad():  # No need to track gradients in validation phase\n",
    "#             for i, (val_images, val_masks) in enumerate(tqdm(validation_dataloader_T1)):\n",
    "#                 val_images = val_images.to(device)\n",
    "#                 val_masks = val_masks.to(device)\n",
    "\n",
    "#                 val_outputs = model(val_images)\n",
    "#                 val_probs = F.softmax(val_outputs, dim=1)\n",
    "#                 val_masks_one_hot = F.one_hot(val_masks, num_classes = 4).permute(0, 3, 1, 2).contiguous().float()\n",
    "#                 val_loss = criterion(val_probs, val_masks_one_hot)\n",
    "                \n",
    "#                 # val_loss = criterion(val_outputs, val_masks)\n",
    "                \n",
    "#                 running_val_loss += val_loss.item()\n",
    "#                 num_val_batches += 1\n",
    "\n",
    "\n",
    "#                 if epoch % 1 == 0 and i == 0:\n",
    "#                     print(\"Validation Prediction Visualization:\")\n",
    "#                     _, preds = torch.max(val_outputs[:2], 1)\n",
    "                    \n",
    "#                     plot_preds_and_masks(preds, val_masks[:2])\n",
    "\n",
    "\n",
    "#                 if epoch % 1 == 0:\n",
    "#                     _, val_preds = torch.max(val_outputs, 1)\n",
    "#                     val_preds_one_hot = F.one_hot(val_preds, num_classes = 4)\n",
    "#                     val_preds_one_hot = val_preds_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "#                     val_masks_one_hot = F.one_hot(val_masks, num_classes = 4)\n",
    "#                     val_masks_one_hot = val_masks_one_hot.permute(0, 3, 1, 2)\n",
    "                    \n",
    "#                     val_dsc, val_assd = compute_metrics(val_preds_one_hot.cpu().numpy(), val_masks_one_hot.cpu().numpy())\n",
    "#                     running_val_dsc.append(val_dsc)\n",
    "#                     running_val_assd.append(val_assd)\n",
    "\n",
    "#         # Compute and print the average validation loss for this epoch\n",
    "#         avg_val_loss = running_val_loss / num_val_batches\n",
    "#         print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "#         if epoch % 1 == 0:\n",
    "#             print(f\"\\nAverage Validation DSC: {np.mean(np.array(running_val_dsc), axis = 0)}\\nAverage Validation ASSD: {np.mean(np.array(running_val_assd), axis = 0)}\")\n",
    "\n",
    "#         # Save the trained model\n",
    "#         torch.save(model.state_dict(), \"unet_model_att.pth\")\n",
    "\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c24ca62-6939-4fb0-a36f-4e319f1f77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(num_epochs):\n",
    "    # Create dataset and dataloader for T1\n",
    "#     training_dataset_T1 = train_loader\n",
    "#     validation_dataset_T1 = val_loader\n",
    "\n",
    "    training_dataloader_T1 = train_loader\n",
    "    validation_dataloader_T1 = val_loader\n",
    "\n",
    "    # Check if saved model exists, if so load it, otherwise initialize a new one\n",
    "    model_path = \"unet_model__multi_attn.pth\"\n",
    "    if os.path.isfile(model_path):\n",
    "        print(\"Loading model...\")\n",
    "        model = AttentionUnet(in_channels = 2, out_channels = 4)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(device)\n",
    "        print(\"Model loaded.\")\n",
    "    else:\n",
    "        print(\"Initializing new model...\")\n",
    "        model = AttentionUnet(in_channels = 2, out_channels = 4).to(device)\n",
    "        print(\"New model initialized.\")\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.00002)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch { epoch + 1 }/{ num_epochs }\")\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "        # Initialize the running loss to zero\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Initialize counter for the number of batches\n",
    "        num_batches = 0\n",
    "\n",
    "        # Initialize the running DSC and ASSD\n",
    "        running_train_dsc = []\n",
    "        running_train_assd = []\n",
    "\n",
    "        # Training phase\n",
    "        model.train() \n",
    "        for i, (images, masks) in enumerate(tqdm(training_dataloader_T1)):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "#             masks = masks.float().squeeze(1).permute(0, 3, 2, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 1 == 0 and i == 0:\n",
    "                print(\"Training Prediction Visualization:\")\n",
    "                _, preds = torch.max(outputs[:2], 1)\n",
    "#                 print(outputs.shape,masks.shape)\n",
    "                plot_preds_and_masks(preds,masks[:2])\n",
    "\n",
    "            # Update the running loss and batch count\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                preds_one_hot = F.one_hot(preds, num_classes = 4)\n",
    "                preds_one_hot = preds_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "                masks_one_hot = F.one_hot(masks, num_classes = 4)\n",
    "                masks_one_hot = masks_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "                train_dsc, train_assd = compute_metrics(preds_one_hot.cpu().numpy(), masks_one_hot.cpu().numpy())\n",
    "                running_train_dsc.append(train_dsc)\n",
    "                running_train_assd.append(train_assd)\n",
    "\n",
    "\n",
    "        # Compute and print the average training loss for this epoch\n",
    "        avg_train_loss = running_loss / num_batches\n",
    "        print(f\"\\nAverage Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"\\nAverage Training DSC: {np.mean(np.array(running_train_dsc), axis = 0)}\\nAverage Training ASSD: {np.mean(np.array(running_train_assd), axis = 0)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        running_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        running_val_dsc = []\n",
    "        running_val_assd = []\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients in validation phase\n",
    "            for i, (val_images, val_masks) in enumerate(tqdm(validation_dataloader_T1)):\n",
    "                val_images = val_images.to(device)\n",
    "                val_masks = val_masks.to(device)\n",
    "\n",
    "                val_outputs = model(val_images)\n",
    "                val_loss = criterion(val_outputs, val_masks)\n",
    "                \n",
    "                running_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "\n",
    "                if epoch % 1 == 0 and i == 0:\n",
    "                    print(\"Validation Prediction Visualization:\")\n",
    "                    _, preds = torch.max(val_outputs[:2], 1)\n",
    "                    \n",
    "                    plot_preds_and_masks(preds, val_masks[:2])\n",
    "\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    _, val_preds = torch.max(val_outputs, 1)\n",
    "                    val_preds_one_hot = F.one_hot(val_preds, num_classes = 4)\n",
    "                    val_preds_one_hot = val_preds_one_hot.permute(0, 3, 1, 2)\n",
    "\n",
    "                    val_masks_one_hot = F.one_hot(val_masks, num_classes = 4)\n",
    "                    val_masks_one_hot = val_masks_one_hot.permute(0, 3, 1, 2)\n",
    "                    \n",
    "                    val_dsc, val_assd = compute_metrics(val_preds_one_hot.cpu().numpy(), val_masks_one_hot.cpu().numpy())\n",
    "                    running_val_dsc.append(val_dsc)\n",
    "                    running_val_assd.append(val_assd)\n",
    "\n",
    "        # Compute and print the average validation loss for this epoch\n",
    "        avg_val_loss = running_val_loss / num_val_batches\n",
    "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"\\nAverage Validation DSC: {np.mean(np.array(running_val_dsc), axis = 0)}\\nAverage Validation ASSD: {np.mean(np.array(running_val_assd), axis = 0)}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        torch.save(model.state_dict(), \"unet_model_multi_attn.pth\")\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a527dc06-5a60-4ed2-b073-6aa1e9851ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b74f4001-9341-4610-af1a-2c231a8f48db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c33070d1-a44a-4886-b63d-096bf808005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model...\n",
      "New model initialized.\n",
      "Epoch 1/50\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2215 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_NOT_INITIALIZED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2146695/1681750547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2146695/3419564009.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2146695/3834232739.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0me2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2146695/551606551.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d73d5-de1d-4732-9a31-227b3f673f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-2022.10]",
   "language": "python",
   "name": "conda-env-anaconda-2022.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
