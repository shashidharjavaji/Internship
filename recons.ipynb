{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271eb9c3-e21d-45c4-91dd-78bf254d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from medpy.metric import dc, assd\n",
    "import torch.nn.functional as F\n",
    "import torchio as tio\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94776590-bdff-4d9d-a458-3084e00ecc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Downsample path\n",
    "        self.conv1 = self.double_conv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = self.double_conv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = self.double_conv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = self.double_conv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottom\n",
    "        self.conv5 = self.double_conv(512, 1024)\n",
    "\n",
    "        # Upsample path\n",
    "        self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size = 2, stride = 2)\n",
    "        self.conv6 = self.double_conv(1024, 512)\n",
    "        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size = 2, stride = 2)\n",
    "        self.conv7 = self.double_conv(512, 256)\n",
    "        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size = 2, stride = 2)\n",
    "        self.conv8 = self.double_conv(256, 128)\n",
    "        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size = 2, stride = 2)\n",
    "        self.conv9 = self.double_conv(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.conv10 = nn.Conv2d(64, out_channels, kernel_size = 1)\n",
    "\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsample path\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "\n",
    "        # Bottom\n",
    "        c5 = self.conv5(p4)\n",
    "\n",
    "        # Upsample path\n",
    "        # up6 = self.up6(c5)\n",
    "        up6 = F.interpolate(self.up6(c5), size=c4.size()[2:], mode='bilinear', align_corners=False)\n",
    "        merge6 = torch.cat([up6, c4], dim = 1)\n",
    "        c6 = self.conv6(merge6)\n",
    "\n",
    "        up7 = F.interpolate(self.up7(c6), size=c3.size()[2:], mode='bilinear', align_corners=False)\n",
    "        merge7 = torch.cat([up7, c3], dim = 1)\n",
    "        c7 = self.conv7(merge7)\n",
    "\n",
    "        up8 = F.interpolate(self.up8(c7), size=c2.size()[2:], mode='bilinear', align_corners=False)\n",
    "        merge8 = torch.cat([up8, c2], dim = 1)\n",
    "        c8 = self.conv8(merge8)\n",
    "\n",
    "        up9 = F.interpolate(self.up9(c8), size=c1.size()[2:], mode='bilinear', align_corners=False)\n",
    "        merge9 = torch.cat([up9, c1], dim = 1)\n",
    "        c9 = self.conv9(merge9)\n",
    "\n",
    "        # Output\n",
    "        out = self.conv10(c9)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        out = self.forward(x)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecc3dba-9784-4a53-85d1-0328e357663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(slice_2d):\n",
    "    max_val = np.max(slice_2d)\n",
    "    min_val = np.min(slice_2d)\n",
    "    # Only normalize if there is data\n",
    "    if max_val - min_val > 0:\n",
    "        slice_2d_normalized = (slice_2d - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        assert max_val == 0 and min_val == 0\n",
    "        slice_2d_normalized = slice_2d\n",
    "    return slice_2d_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3c20d1-890d-48c9-96e8-e6c82b78dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient: BraTS-PED-00002-000-t1c.nii.gz\n",
      "Processing patient: BraTS-PED-00003-000-t1c.nii.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4180888/4236395985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Convert to tensors and stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1c_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2f_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# stack = torch.stack([ToTensor()(t1c_img), ToTensor()(t2f_img)]).unsqueeze(0).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# print(stack.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor\n",
    "device = \"cuda:2\"\n",
    "# Create an instance of your model\n",
    "model = UNet(in_channels=2, out_channels=4)\n",
    "model.load_state_dict(torch.load(\"unet_PED_axial.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# Define the directories\n",
    "t1c_dir = \"pediatric_modalities/t1c\"\n",
    "t2f_dir = \"pediatric_modalities/t2f\"\n",
    "output_dir =  \"PED_ax_op\"\n",
    "original_seg_dir = \"ASNR-MICCAI-BraTS2023-PED-Challenge-TrainingData/\"\n",
    "\n",
    "# Iterate over patient folders\n",
    "for patient_dir in sorted(os.listdir(t1c_dir)):\n",
    "    print(f\"Processing patient: {patient_dir}\")\n",
    "    \n",
    "    t1c_img = nib.load(os.path.join(t1c_dir, patient_dir)).get_fdata()\n",
    "    t2f_img = nib.load(os.path.join(t2f_dir, patient_dir.replace(\"t1c\", \"t2f\"))).get_fdata()\n",
    "    \n",
    "    \n",
    "    # Placeholder for the output segmentation slices\n",
    "    seg_slices = []\n",
    "    \n",
    "    \n",
    "    for i in range(t1c_img.shape[-1]):\n",
    "        t1c_img_slice = min_max_normalization(t1c_img[:, :, i])\n",
    "        t2f_img_slice = min_max_normalization(t2f_img[:, :, i])\n",
    "        \n",
    "        t1c_tensor = torch.from_numpy(np.array(t1c_img_slice,dtype = np.float32)[None, ...])\n",
    "        \n",
    "        t2f_tensor = torch.from_numpy(np.array(t2f_img_slice,dtype = np.float32)[None, ...])\n",
    "        \n",
    "        # Convert to tensors and stack\n",
    "        stack = torch.cat((t1c_tensor, t2f_tensor), dim=0).unsqueeze(0).to(device)\n",
    "        output = model(stack)\n",
    "\n",
    "        _, preds = torch.max(output, 1)\n",
    "        \n",
    "        # Save the segmentation slice\n",
    "        seg_slices.append(preds.squeeze().detach().cpu().numpy())\n",
    "    \n",
    "    # Stack the segmentation slices to form a 3D image\n",
    "    seg_3d = np.stack(seg_slices, axis=-1)\n",
    "    \n",
    "    # Load a original segmentation file to get the affine and header\n",
    "    original_seg_file = nib.load(os.path.join(original_seg_dir, patient_dir.replace('-t1c', '').split('.')[0], f\"{patient_dir.replace('-t1c', '').split('.')[0]}-seg.nii.gz\"))\n",
    "    \n",
    "    # Resize and reorient the 3D predicted mask\n",
    "    # target_dims = original_seg_file.shape\n",
    "    # transform = tio.CropOrPad(target_dims)\n",
    "    # seg_3d = torch.from_numpy(seg_3d)\n",
    "    # seg_3d = seg_3d.unsqueeze(0)\n",
    "    # seg_3d = transform(seg_3d)\n",
    "    # seg_3d = seg_3d.squeeze(0).numpy()\n",
    "\n",
    "    # Convert the 3D image to a Nifti1Image and save\n",
    "    seg_nifti = nib.Nifti1Image(seg_3d, affine=original_seg_file.affine, header=original_seg_file.header)\n",
    "    nib.save(seg_nifti, os.path.join(output_dir, f\"{patient_dir}_segmentation.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6475f0-c8cc-4810-9bf1-59d74b5768ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor\n",
    "device = \"cuda:2\"\n",
    "# Create an instance of your model\n",
    "model = UNet(in_channels=2, out_channels=4)\n",
    "model.load_state_dict(torch.load(\"unet_PED_coronal.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# Define the directories\n",
    "t1c_dir = \"pediatric_modalities/t1c\"\n",
    "t2f_dir = \"pediatric_modalities/t2f\"\n",
    "output_dir =  \"PED_ax_op\"\n",
    "original_seg_dir = \"ASNR-MICCAI-BraTS2023-PED-Challenge-TrainingData/\"\n",
    "\n",
    "# Iterate over patient folders\n",
    "for patient_dir in sorted(os.listdir(t1c_dir)):\n",
    "    print(f\"Processing patient: {patient_dir}\")\n",
    "    \n",
    "    t1c_img = nib.load(os.path.join(t1c_dir, patient_dir)).get_fdata()\n",
    "    t2f_img = nib.load(os.path.join(t2f_dir, patient_dir.replace(\"t1c\", \"t2f\"))).get_fdata()\n",
    "    \n",
    "    \n",
    "    # Placeholder for the output segmentation slices\n",
    "    seg_slices = []\n",
    "    \n",
    "    \n",
    "    for i in range(t1c_img.shape[-1]):\n",
    "        t1c_img_slice = min_max_normalization(t1c_img[:, i, :])\n",
    "        t2f_img_slice = min_max_normalization(t2f_img[:, i, :])\n",
    "        \n",
    "        t1c_tensor = torch.from_numpy(np.array(t1c_img_slice,dtype = np.float32)[None, ...])\n",
    "        \n",
    "        t2f_tensor = torch.from_numpy(np.array(t2f_img_slice,dtype = np.float32)[None, ...])\n",
    "        \n",
    "        # Convert to tensors and stack\n",
    "        stack = torch.cat((t1c_tensor, t2f_tensor), dim=0).unsqueeze(0).to(device)\n",
    "        # stack = torch.stack([ToTensor()(t1c_img), ToTensor()(t2f_img)]).unsqueeze(0).to(device)\n",
    "        # print(stack.size())\n",
    "        # stack = stack.view(stack.shape[0], stack.shape[1], stack.shape[3], stack.shape[4])  # Reshape here\n",
    "        output = model(stack)\n",
    "\n",
    "        _, preds = torch.max(output, 1)\n",
    "        \n",
    "        # Save the segmentation slice\n",
    "        seg_slices.append(preds.squeeze().detach().cpu().numpy())\n",
    "    \n",
    "    # Stack the segmentation slices to form a 3D image\n",
    "    seg_3d = np.stack(seg_slices, axis=1)\n",
    "    \n",
    "    # Load a original segmentation file to get the affine and header\n",
    "    original_seg_file = nib.load(os.path.join(original_seg_dir, patient_dir.replace('-t1c', '').split('.')[0], f\"{patient_dir.replace('-t1c', '').split('.')[0]}-seg.nii.gz\"))\n",
    "    \n",
    "    # Resize and reorient the 3D predicted mask\n",
    "    # target_dims = original_seg_file.shape\n",
    "    # transform = tio.CropOrPad(target_dims)\n",
    "    # seg_3d = torch.from_numpy(seg_3d)\n",
    "    # seg_3d = seg_3d.unsqueeze(0)\n",
    "    # seg_3d = transform(seg_3d)\n",
    "    # seg_3d = seg_3d.squeeze(0).numpy()\n",
    "\n",
    "    # Convert the 3D image to a Nifti1Image and save\n",
    "    seg_nifti = nib.Nifti1Image(seg_3d, affine=original_seg_file.affine, header=original_seg_file.header)\n",
    "    nib.save(seg_nifti, os.path.join(output_dir, f\"{patient_dir}_segmentation.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54c1a6-717c-4b13-84cf-c311faefda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor\n",
    "device = \"cuda:2\"\n",
    "# Create an instance of your model\n",
    "model = UNet(in_channels=2, out_channels=4)\n",
    "model.load_state_dict(torch.load(\"unet_PED_sagittal.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# Define the directories\n",
    "t1c_dir = \"pediatric_modalities/t1c\"\n",
    "t2f_dir = \"pediatric_modalities/t2f\"\n",
    "output_dir =  \"PED_ax_op\"\n",
    "original_seg_dir = \"ASNR-MICCAI-BraTS2023-PED-Challenge-TrainingData/\"\n",
    "\n",
    "# Iterate over patient folders\n",
    "for patient_dir in sorted(os.listdir(t1c_dir)):\n",
    "    print(f\"Processing patient: {patient_dir}\")\n",
    "    \n",
    "    t1c_img = nib.load(os.path.join(t1c_dir, patient_dir)).get_fdata()\n",
    "    t2f_img = nib.load(os.path.join(t2f_dir, patient_dir.replace(\"t1c\", \"t2f\"))).get_fdata()\n",
    "    \n",
    "    \n",
    "    # Placeholder for the output segmentation slices\n",
    "    seg_slices = []\n",
    "    \n",
    "    \n",
    "    for i in range(t1c_img.shape[-1]):\n",
    "        t1c_img_slice = min_max_normalization(t1c_img[i, :, :])\n",
    "        t2f_img_slice = min_max_normalization(t2f_img[i, :, :])\n",
    "        \n",
    "        t1c_tensor = torch.from_numpy(np.array(t1c_img_slice,dtype = np.float32)[None, ...])\n",
    "        \n",
    "        t2f_tensor = torch.from_numpy(np.array(t2f_img_slice,dtype = np.float32)[None, ...])\n",
    "        \n",
    "        # Convert to tensors and stack\n",
    "        stack = torch.cat((t1c_tensor, t2f_tensor), dim=0).unsqueeze(0).to(device)\n",
    "        # stack = torch.stack([ToTensor()(t1c_img), ToTensor()(t2f_img)]).unsqueeze(0).to(device)\n",
    "        # print(stack.size())\n",
    "        # stack = stack.view(stack.shape[0], stack.shape[1], stack.shape[3], stack.shape[4])  # Reshape here\n",
    "        output = model(stack)\n",
    "\n",
    "        _, preds = torch.max(output, 1)\n",
    "        \n",
    "        # Save the segmentation slice\n",
    "        seg_slices.append(preds.squeeze().detach().cpu().numpy())\n",
    "    \n",
    "    # Stack the segmentation slices to form a 3D image\n",
    "    seg_3d = np.stack(seg_slices, axis=0)\n",
    "    \n",
    "    # Load a original segmentation file to get the affine and header\n",
    "    original_seg_file = nib.load(os.path.join(original_seg_dir, patient_dir.replace('-t1c', '').split('.')[0], f\"{patient_dir.replace('-t1c', '').split('.')[0]}-seg.nii.gz\"))\n",
    "    \n",
    "    # Resize and reorient the 3D predicted mask\n",
    "    # target_dims = original_seg_file.shape\n",
    "    # transform = tio.CropOrPad(target_dims)\n",
    "    # seg_3d = torch.from_numpy(seg_3d)\n",
    "    # seg_3d = seg_3d.unsqueeze(0)\n",
    "    # seg_3d = transform(seg_3d)\n",
    "    # seg_3d = seg_3d.squeeze(0).numpy()\n",
    "\n",
    "    # Convert the 3D image to a Nifti1Image and save\n",
    "    seg_nifti = nib.Nifti1Image(seg_3d, affine=original_seg_file.affine, header=original_seg_file.header)\n",
    "    nib.save(seg_nifti, os.path.join(output_dir, f\"{patient_dir}_segmentation.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfe046-5694-4bd2-82b2-000086df999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load your pre-trained models' predictions\n",
    "axial_preds = img1.get_fdata()\n",
    "coronal_preds = img2.get_fdata()\n",
    "sagittal_preds = img3.get_fdata()\n",
    "\n",
    "# Let's assume the dimensions of the original MRI are:\n",
    "original_shape = (240, 240, 155)  # Adjust as per your data\n",
    "\n",
    "# Rescale the predictions to the original dimensions:\n",
    "rescaled_axial = axial_preds\n",
    "rescaled_coronal = coronal_preds\n",
    "rescaled_sagittal = sagittal_preds\n",
    "\n",
    "# Combine the three predictions\n",
    "combined_preds = np.stack([rescaled_axial, rescaled_coronal, rescaled_sagittal])\n",
    "\n",
    "# Create the final segmentation by taking the majority vote\n",
    "final_segmentation = mode(combined_preds, axis=0)[0][0]\n",
    "\n",
    "# Create a Nifti image from the final segmentation\n",
    "# In your case, you might want to use the affine and header from one of the loaded NIfTI images\n",
    "final_seg_nifti = nib.Nifti1Image(final_segmentation, affine=img1.affine, header=img1.header)\n",
    "\n",
    "# Save the final segmentation\n",
    "nib.save(final_seg_nifti, \"final_segmentation.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34427d45-79f6-415d-b888-0fa498835395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda:2\"\n",
    "model = UNet(in_channels=2, out_channels=4)\n",
    "model = model.to(device)\n",
    "\n",
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c3ee360-6784-4311-8d50-1afd75bbfeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient: BraTS-PED-00002-000-t1c.nii.gz\n",
      "Processing patient: BraTS-PED-00003-000-t1c.nii.gz\n",
      "Processing patient: BraTS-PED-00004-000-t1c.nii.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7782/1116958050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Run the function for each orientation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0morientation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"coronal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axial\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sagittal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mprocess_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7782/1116958050.py\u001b[0m in \u001b[0;36mprocess_orientation\u001b[0;34m(orientation)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Save the segmentation slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mseg_slices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Stack the segmentation slices to form a 3D image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_orientation(orientation):\n",
    "    if orientation == \"axial\":\n",
    "        model.load_state_dict(torch.load(\"unet_PED_axial.pth\"))\n",
    "    elif orientation == \"coronal\":\n",
    "        model.load_state_dict(torch.load(\"unet_PED_coronal.pth\"))\n",
    "    else:  # orientation == \"sagittal\"\n",
    "        model.load_state_dict(torch.load(\"unet_PED_sagittal.pth\"))\n",
    "\n",
    "    # Define the directories\n",
    "    t1c_dir = \"pediatric_modalities/t1c\"\n",
    "    t2f_dir = \"pediatric_modalities/t2f\"\n",
    "    output_dir =  \"PED_ax_op\"\n",
    "    original_seg_dir = \"ASNR-MICCAI-BraTS2023-PED-Challenge-TrainingData/\"\n",
    "\n",
    "    # Iterate over patient folders\n",
    "    for patient_dir in sorted(os.listdir(t1c_dir), key=natural_keys):\n",
    "        print(f\"Processing patient: {patient_dir}\")\n",
    "\n",
    "        t1c_img = nib.load(os.path.join(t1c_dir, patient_dir)).get_fdata()\n",
    "        t2f_img = nib.load(os.path.join(t2f_dir, patient_dir.replace(\"t1c\", \"t2f\"))).get_fdata()\n",
    "\n",
    "        # Placeholder for the output segmentation slices\n",
    "        seg_slices = []\n",
    "        \n",
    "        if orientation == \"axial\":\n",
    "            shape = t1c_img.shape[-1]\n",
    "        elif orientation == \"coronal\":\n",
    "            shape = t1c_img.shape[1]\n",
    "        else:\n",
    "            shape = t1c_img.shape[0]\n",
    "        \n",
    "        for i in range(shape):\n",
    "            if orientation == \"axial\":\n",
    "                t1c_img_slice = min_max_normalization(t1c_img[:, :, i])\n",
    "                t2f_img_slice = min_max_normalization(t2f_img[:, :, i])\n",
    "            elif orientation == \"coronal\":\n",
    "                t1c_img_slice = min_max_normalization(t1c_img[:, i, :])\n",
    "                t2f_img_slice = min_max_normalization(t2f_img[:, i, :])\n",
    "            else:  # orientation == \"sagittal\"\n",
    "                t1c_img_slice = min_max_normalization(t1c_img[i, :, :])\n",
    "                t2f_img_slice = min_max_normalization(t2f_img[i, :, :])\n",
    "\n",
    "            t1c_tensor = torch.from_numpy(np.array(t1c_img_slice,dtype = np.float32)[None, ...])\n",
    "\n",
    "            t2f_tensor = torch.from_numpy(np.array(t2f_img_slice,dtype = np.float32)[None, ...])\n",
    "\n",
    "            # Convert to tensors and stack\n",
    "            stack = torch.cat((t1c_tensor, t2f_tensor), dim=0).unsqueeze(0).to(device)\n",
    "            output = model(stack)\n",
    "\n",
    "            _, preds = torch.max(output, 1)\n",
    "\n",
    "            # Save the segmentation slice\n",
    "            seg_slices.append(preds.squeeze().detach().cpu().numpy())\n",
    "\n",
    "        # Stack the segmentation slices to form a 3D image\n",
    "        if orientation == \"axial\":\n",
    "            seg_3d = np.stack(seg_slices, axis=-1)\n",
    "        elif orientation == \"coronal\":\n",
    "            seg_3d = np.stack(seg_slices, axis=1)\n",
    "        else:  # orientation == \"sagittal\"\n",
    "            seg_3d = np.stack(seg_slices, axis=0)\n",
    "\n",
    "        # Load a original segmentation file to get the affine and header\n",
    "        original_seg_file = nib.load(os.path.join(original_seg_dir, patient_dir.replace('-t1c', '').split('.')[0], f\"{patient_dir.replace('-t1c', '').split('.')[0]}-seg.nii.gz\"))\n",
    "\n",
    "        # Convert the 3D image to a Nifti1Image and save\n",
    "        seg_nifti = nib.Nifti1Image(seg_3d, affine=original_seg_file.affine, header=original_seg_file.header)\n",
    "        nib.save(seg_nifti, os.path.join(output_dir, f\"{patient_dir}_{orientation}_segmentation.nii.gz\"))\n",
    "\n",
    "# Run the function for each orientation\n",
    "for orientation in [\"coronal\", \"axial\", \"sagittal\"]:\n",
    "    process_orientation(orientation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09328320-c448-4d5e-a421-b1f048ccc6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb1e26-8d6f-4a39-9709-c3e5fe25e9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c6412-f3d9-47d8-87bd-1722ec245d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab954768-7db5-487b-abf4-e13730c5d21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2da644-9f04-48ca-9ab2-7101f4f4fbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b82158-91c4-4045-9f8e-7ee41bb56d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff03fad-f201-4a13-b963-f179e36cb2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23100c3d-bf95-4a63-94cf-b40a2534ddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1071b-9ba8-4a70-980b-992f769bfd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0f2f8-3880-4db1-bbac-e48754323789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e14178-3b56-4b3d-88f4-bffcb8b9a73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e10ad-7b4f-4d57-b622-655830f774cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8f839f-1f6e-496a-b63c-e3401224afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'PED_Sliced/axial/t2f/BraTS-PED-00002-000/.ipynb_checkpoints' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "directory_path = 'PED_Sliced/axial/t2f/BraTS-PED-00002-000/.ipynb_checkpoints'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory_path) and os.path.isdir(directory_path):\n",
    "    # Delete the directory\n",
    "    shutil.rmtree(directory_path)\n",
    "    print(f\"The directory '{directory_path}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"The directory '{directory_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f10136-8707-4c84-93a9-962c312e4cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "sequence argument must have length equal to input rank",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4152552/2911868723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mseg_affine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_nii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mreconstructed_3D_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruct_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoronal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxial_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagittal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4152552/2911868723.py\u001b[0m in \u001b[0;36mreconstruct_3D\u001b[0;34m(coronal, axial, sagittal, target_shape)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mzoom_factors_sagittal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget_dim\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcurrent_dim\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msagittal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mstack_coronal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoronal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_factors_coronal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mstack_axial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_factors_axial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mstack_sagittal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msagittal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_factors_sagittal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/modules/apps/anaconda/2022.10/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input and output rank must be > 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     output_shape = tuple(\n\u001b[1;32m    769\u001b[0m             [int(round(ii * jj)) for ii, jj in zip(input.shape, zoom)])\n",
      "\u001b[0;32m/modules/apps/anaconda/2022.10/lib/python3.9/site-packages/scipy/ndimage/_ni_support.py\u001b[0m in \u001b[0;36m_normalize_sequence\u001b[0;34m(input, rank)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sequence argument must have length equal to input rank\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: sequence argument must have length equal to input rank"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the models\n",
    "model_coronal = UNet(in_channels=2, out_channels=4).to(device)\n",
    "model_coronal.load_state_dict(torch.load('unet_PED_coronal.pth'))\n",
    "model_coronal.eval()\n",
    "\n",
    "model_axial = UNet(in_channels=2, out_channels=4).to(device)\n",
    "model_axial.load_state_dict(torch.load('unet_PED_axial.pth'))\n",
    "model_axial.eval()\n",
    "\n",
    "model_sagittal = UNet(in_channels=2, out_channels=4).to(device)\n",
    "model_sagittal.load_state_dict(torch.load('unet_PED_sagittal.pth'))\n",
    "model_sagittal.eval()\n",
    "\n",
    "# Define the function to load the slices\n",
    "def load_slices(t1c_folder, t2f_folder):\n",
    "    t1c_files = sorted(os.listdir(t1c_folder))\n",
    "    t2f_files = sorted(os.listdir(t2f_folder))\n",
    "\n",
    "    slices = []\n",
    "    for t1c_file, t2f_file in zip(t1c_files, t2f_files):\n",
    "        t1c_slice = np.array(Image.open(os.path.join(t1c_folder, t1c_file)))\n",
    "        t2f_slice = np.array(Image.open(os.path.join(t2f_folder, t2f_file)))\n",
    "        \n",
    "        # Stack the t1c and t2f slices along the channel dimension and normalize\n",
    "        stacked_slice = np.stack((t1c_slice, t2f_slice), axis=0) / 255.0\n",
    "        slices.append(stacked_slice)\n",
    "\n",
    "    return slices\n",
    "\n",
    "def get_prediction(model, data):\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        preds = model.predict(data)\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def reconstruct_3D(coronal, axial, sagittal, target_shape):\n",
    "    # Reshape each stack to the target shape\n",
    "    zoom_factors_coronal = [target_dim / current_dim for target_dim, current_dim in zip(target_shape, np.stack(coronal, axis=1).shape)]\n",
    "    zoom_factors_axial = [target_dim / current_dim for target_dim, current_dim in zip(target_shape, np.stack(axial, axis=0).shape)]\n",
    "    zoom_factors_sagittal = [target_dim / current_dim for target_dim, current_dim in zip(target_shape, np.swapaxes(np.stack(sagittal, axis=2), 0, 1).shape)]\n",
    "    \n",
    "    stack_coronal = zoom(np.stack(coronal, axis=1), zoom_factors_coronal)\n",
    "    stack_axial = zoom(np.stack(axial, axis=0), zoom_factors_axial)\n",
    "    stack_sagittal = zoom(np.swapaxes(np.stack(sagittal, axis=2), 0, 1), zoom_factors_sagittal)\n",
    "\n",
    "    assert stack_coronal.shape == stack_axial.shape == stack_sagittal.shape == target_shape\n",
    "\n",
    "    stack_list = [stack_coronal, stack_axial, stack_sagittal]\n",
    "\n",
    "    final_stack = np.empty(target_shape, dtype=np.int)\n",
    "\n",
    "    for i in range(target_shape[0]):\n",
    "        for j in range(target_shape[1]):\n",
    "            for k in range(target_shape[2]):\n",
    "                voxel_values = [stack[i, j, k] for stack in stack_list]\n",
    "                final_stack[i, j, k] = max(set(voxel_values), key=voxel_values.count)\n",
    "\n",
    "    return final_stack\n",
    "\n",
    "# Root folders\n",
    "axial_t1c_root = \"PED_Sliced/axial/t1c/\"\n",
    "axial_t2f_root = \"PED_Sliced/axial/t2f/\"\n",
    "sagittal_t1c_root = \"PED_Sliced/sagittal/t1c/\"\n",
    "sagittal_t2f_root = \"PED_Sliced/sagittal/t2f/\"\n",
    "coronal_t1c_root = \"PED_Sliced/coronal/t1c/\"\n",
    "coronal_t2f_root = \"PED_Sliced/coronal/t2f/\"\n",
    "seg_root = \"pediatric_modalities/seg/\"\n",
    "output_folder = 'PED_Sliced/reconstructed/'\n",
    "\n",
    "# For each patient folder\n",
    "for patient_folder in sorted(os.listdir(axial_t1c_root)):\n",
    "    axial_t1c_folder = os.path.join(axial_t1c_root, patient_folder)\n",
    "    axial_t2f_folder = os.path.join(axial_t2f_root, patient_folder)\n",
    "    sagittal_t1c_folder = os.path.join(sagittal_t1c_root, patient_folder)\n",
    "    sagittal_t2f_folder = os.path.join(sagittal_t2f_root, patient_folder)\n",
    "    coronal_t1c_folder = os.path.join(coronal_t1c_root, patient_folder)\n",
    "    coronal_t2f_folder = os.path.join(coronal_t2f_root, patient_folder)\n",
    "    seg_file = os.path.join(seg_root, patient_folder + '-seg.nii.gz')\n",
    "\n",
    "    axial_slices = load_slices(axial_t1c_folder, axial_t2f_folder)\n",
    "    sagittal_slices = load_slices(sagittal_t1c_folder, sagittal_t2f_folder)\n",
    "    coronal_slices = load_slices(coronal_t1c_folder, coronal_t2f_folder)\n",
    "\n",
    "    axial_preds = [get_prediction(model_axial, torch.tensor([slice], dtype=torch.float32)) for slice in axial_slices]\n",
    "    sagittal_preds = [get_prediction(model_sagittal, torch.tensor([slice], dtype=torch.float32)) for slice in sagittal_slices]\n",
    "    coronal_preds = [get_prediction(model_coronal, torch.tensor([slice], dtype=torch.float32)) for slice in coronal_slices]\n",
    "\n",
    "    # reconstructed_3D_image = reconstruct_3D(coronal_preds, axial_preds, sagittal_preds)\n",
    "\n",
    "    # Use the affine from the original segmentation\n",
    "    seg_nii = nib.load(seg_file)\n",
    "    target_shape = seg_nii.shape\n",
    "    seg_affine = seg_nii.affine\n",
    "    \n",
    "    reconstructed_3D_image = reconstruct_3D(coronal_preds, axial_preds, sagittal_preds, target_shape)\n",
    "\n",
    "\n",
    "    # Save the reconstructed 3D image\n",
    "    output_filename = os.path.join(output_folder, patient_folder + '.nii.gz')\n",
    "    img_nifti = nib.Nifti1Image(reconstructed_3D_image, seg_affine)\n",
    "    nib.save(img_nifti, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f1dddf-988f-4644-a959-bcb1170672ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "sequence argument must have length equal to input rank",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4152552/3213823680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mtarget_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m155\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mreconstructed_3D_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruct_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoronal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxial_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagittal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Saving the output 3D image as a nii.gz file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4152552/3213823680.py\u001b[0m in \u001b[0;36mreconstruct_3D\u001b[0;34m(coronal, axial, sagittal, target_shape)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreconstruct_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoronal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagittal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Reshape each stack to the target shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mstack_coronal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoronal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mstack_axial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mstack_sagittal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msagittal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/modules/apps/anaconda/2022.10/lib/python3.9/site-packages/scipy/ndimage/_interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input and output rank must be > 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     output_shape = tuple(\n\u001b[1;32m    769\u001b[0m             [int(round(ii * jj)) for ii, jj in zip(input.shape, zoom)])\n",
      "\u001b[0;32m/modules/apps/anaconda/2022.10/lib/python3.9/site-packages/scipy/ndimage/_ni_support.py\u001b[0m in \u001b[0;36m_normalize_sequence\u001b[0;34m(input, rank)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sequence argument must have length equal to input rank\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: sequence argument must have length equal to input rank"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import zoom\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Modify the path to the saved models according to your setup\n",
    "model_axial_path = 'unet_PED_axial.pth'\n",
    "model_sagittal_path = 'unet_PED_sagittal.pth'\n",
    "model_coronal_path = 'unet_PED_coronal.pth'\n",
    "\n",
    "# Load your models\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the models\n",
    "model_coronal = UNet(in_channels=2, out_channels=4).to(device)\n",
    "model_coronal.load_state_dict(torch.load('unet_PED_coronal.pth'))\n",
    "model_coronal.eval()\n",
    "\n",
    "model_axial = UNet(in_channels=2, out_channels=4).to(device)\n",
    "model_axial.load_state_dict(torch.load('unet_PED_axial.pth'))\n",
    "model_axial.eval()\n",
    "\n",
    "model_sagittal = UNet(in_channels=2, out_channels=4).to(device)\n",
    "model_sagittal.load_state_dict(torch.load('unet_PED_sagittal.pth'))\n",
    "model_sagittal.eval()\n",
    "\n",
    "# Load the slices for each plane\n",
    "axial_t1c_root = \"PED_Sliced/axial/t1c/\"\n",
    "axial_t2f_root = \"PED_Sliced/axial/t2f/\"\n",
    "sagittal_t1c_root = \"PED_Sliced/sagittal/t1c/\"\n",
    "sagittal_t2f_root = \"PED_Sliced/sagittal/t2f/\"\n",
    "coronal_t1c_root = \"PED_Sliced/coronal/t1c/\"\n",
    "coronal_t2f_root = \"PED_Sliced/coronal/t2f/\"\n",
    "\n",
    "def load_slices(t1c_folder, t2f_folder):\n",
    "    t1c_files = sorted(glob(os.path.join(t1c_folder, '*.png')))\n",
    "    t2f_files = sorted(glob(os.path.join(t2f_folder, '*.png')))\n",
    "    \n",
    "    slices = []\n",
    "    for t1c_file, t2f_file in zip(t1c_files, t2f_files):\n",
    "        t1c_img = Image.open(t1c_file)\n",
    "        t2f_img = Image.open(t2f_file)\n",
    "        \n",
    "        t1c_tensor = transforms.functional.to_tensor(t1c_img).unsqueeze(0)\n",
    "        t2f_tensor = transforms.functional.to_tensor(t2f_img).unsqueeze(0)\n",
    "        \n",
    "        slice = torch.cat((t1c_tensor, t2f_tensor), dim=1)\n",
    "        slices.append(slice.numpy())\n",
    "        \n",
    "    return slices\n",
    "\n",
    "def get_prediction(model, data):\n",
    "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model.predict(data)\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "def reconstruct_3D(coronal, axial, sagittal, target_shape):\n",
    "    # Reshape each stack to the target shape\n",
    "    stack_coronal = zoom(np.stack(coronal, axis=1), target_shape)\n",
    "    stack_axial = zoom(np.stack(axial, axis=0), target_shape)\n",
    "    stack_sagittal = zoom(np.swapaxes(np.stack(sagittal, axis=2), 0, 1), target_shape)\n",
    "    \n",
    "    # Assert all stacks have the same shape\n",
    "    assert stack_coronal.shape == stack_axial.shape == stack_sagittal.shape\n",
    "\n",
    "    # Majority voting\n",
    "    stack_list = [stack_coronal, stack_axial, stack_sagittal]\n",
    "    reconstructed_3D_image = np.stack(stack_list).astype(int)\n",
    "    reconstructed_3D_image = np.argmax(np.bincount(reconstructed_3D_image.reshape(-1), minlength=len(stack_list)).reshape(-1, *stack_coronal.shape), axis=0)\n",
    "    \n",
    "    return reconstructed_3D_image\n",
    "\n",
    "# Getting 2D predictions for each plane\n",
    "axial_slices = load_slices(axial_t1c_folder, axial_t2f_folder)\n",
    "sagittal_slices = load_slices(sagittal_t1c_folder, sagittal_t2f_folder)\n",
    "coronal_slices = load_slices(coronal_t1c_folder, coronal_t2f_folder)\n",
    "\n",
    "axial_preds = [get_prediction(model_axial, slice) for slice in axial_slices]\n",
    "sagittal_preds = [get_prediction(model_sagittal, slice) for slice in sagittal_slices]\n",
    "coronal_preds = [get_prediction(model_coronal, slice) for slice in coronal_slices]\n",
    "\n",
    "# The target shape for the 3D reconstructed image, set to the original image shape\n",
    "target_shape = (240, 240, 155)\n",
    "\n",
    "reconstructed_3D_image = reconstruct_3D(coronal_preds, axial_preds, sagittal_preds, target_shape)\n",
    "\n",
    "# Saving the output 3D image as a nii.gz file\n",
    "output_folder = \"PED_output_folder\"\n",
    "\n",
    "# Use the affine from the original segmentation\n",
    "seg_nii = nib.load(\"pediatric_modalities/seg/BraTS-PED-00002-000-seg.nii.gz\")\n",
    "seg_affine = seg_nii.affine\n",
    "\n",
    "output_img = nib.Nifti1Image(reconstructed_3D_image, affine=seg_affine)\n",
    "nib.save(output_img, os.path.join(output_folder, \"reconstructed_3D_image.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954243dd-6469-4264-b2b2-16840f79a237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-2022.10]",
   "language": "python",
   "name": "conda-env-anaconda-2022.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
