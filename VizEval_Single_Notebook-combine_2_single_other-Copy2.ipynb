{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the code necessary to evaluate and visualise the predictions of a single model. \n",
    "# **Instructions**: Run all cells and enter the number corresponding to your model of choice when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as anim\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "from skimage.transform import resize\n",
    "from skimage.util import montage\n",
    "\n",
    "from IPython.display import Image as show_gif\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from time import sleep\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Meter import dice_coef_metric_per_classes, jaccard_coef_metric_per_classes\n",
    "\n",
    "from utils.BratsDataset import BratsDataset\n",
    "\n",
    "from utils.Meter import BCEDiceLoss\n",
    "\n",
    "from utils.viz_eval_utils import get_dataloaders, compute_scores_per_classes, count_parameters\n",
    "\n",
    "from models.UNet3d import UNet3d\n",
    "from models.UNet3dSingleConv import UNet3dSingleConv\n",
    "from models.UNet3dDropout import UNet3dDropout\n",
    "from models.SwinUNETR import SwinUNETR\n",
    "from models.UNet3d_SELU import UNet3d_SELU\n",
    "from models.UNet3d_atten import UNet3d_atten\n",
    "from models.ONet3d import ONet3d\n",
    "from models.ONet3d_v2 import ONet3d_v2\n",
    "from models.ONet3d_v3 import ONet3d_v3\n",
    "from models.ONet3d_v3_DoubleConv import ONet3d_v3_DoubleConv\n",
    "from models.UNet3d_GELU import UNet3d_GELU\n",
    "from models.ONet3d_v3_GELU import ONet3d_v3_GELU\n",
    "from models.SphereNet3d import SphereNet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_images():\n",
    "#     return list(zip([os.path.join(\"tr_pediatric_modalities/t1c\", image) for image in sorted(os.listdir(\"tr_pediatric_modalities/t1c\"))], [os.path.join(\"tr_pediatric_modalities/t1n\", image) for image in sorted(os.listdir(\"tr_pediatric_modalities/t1n\"))], [os.path.join(\"tr_pediatric_modalities/t2w\", image) for image in sorted(os.listdir(\"tr_pediatric_modalities/t2w\"))], [os.path.join(\"tr_pediatric_modalities/t2f\", image) for image in sorted(os.listdir(\"tr_pediatric_modalities/t2f\"))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images():\n",
    "    return list(zip([os.path.join(\"val_pediatric_modalities/t1c\", image) for image in sorted(os.listdir(\"val_pediatric_modalities/t1c\"))], [os.path.join(\"val_pediatric_modalities/t1n\", image) for image in sorted(os.listdir(\"val_pediatric_modalities/t1n\"))], [os.path.join(\"val_pediatric_modalities/t2w\", image) for image in sorted(os.listdir(\"val_pediatric_modalities/t2w\"))], [os.path.join(\"val_pediatric_modalities/t2f\", image) for image in sorted(os.listdir(\"val_pediatric_modalities/t2f\"))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations import Compose\n",
    "import os\n",
    "\n",
    "class OwnDataset(Dataset):\n",
    "    def __init__(self, do_resizing = False):\n",
    "        self.images = collect_images()\n",
    "        self.do_resizing = do_resizing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = []\n",
    "        paths = self.images[idx]\n",
    "        for i, path in enumerate(paths):\n",
    "            img = self.load_img(path)\n",
    "            \n",
    "            if self.do_resizing:\n",
    "                img = self.resize(img)\n",
    "            \n",
    "            img = self.normalize(img)\n",
    "            images.append(img)\n",
    "        \n",
    "        img = np.stack(images)\n",
    "        \n",
    "        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "        \n",
    "        return torch.from_numpy(img[None, ...])\n",
    "\n",
    "    def load_img(self, file_path):\n",
    "        data = nib.load(file_path)\n",
    "        data = np.asarray(data.dataobj)\n",
    "        return data\n",
    "\n",
    "    def normalize(self, data: np.ndarray):\n",
    "        data_min = np.min(data)\n",
    "        return (data - data_min) / (np.max(data) - data_min)\n",
    "    \n",
    "    def resize(self, data: np.ndarray):\n",
    "        # data = resize(data, (224, 224, 128), preserve_range=True)\n",
    "        data = self.crop_3d_array(data, (224, 224, 128))\n",
    "        return data\n",
    "\n",
    "    def crop_3d_array(self, arr, crop_shape):\n",
    "        \"\"\"\n",
    "        Crop a 3D array to the specified shape.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : numpy.ndarray\n",
    "            The 3D input array to be cropped.\n",
    "        crop_shape : tuple\n",
    "            The shape of the cropped array. Must be a 3-element tuple (depth, height, width).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The cropped array.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(crop_shape) == 3, \"crop_shape must be a 3-element tuple\"\n",
    "        assert crop_shape[0] <= arr.shape[0], \"depth of crop_shape must be <= depth of arr\"\n",
    "        assert crop_shape[1] <= arr.shape[1], \"height of crop_shape must be <= height of arr\"\n",
    "        assert crop_shape[2] <= arr.shape[2], \"width of crop_shape must be <= width of arr\"\n",
    "\n",
    "        depth_diff = arr.shape[0] - crop_shape[0]\n",
    "        height_diff = arr.shape[1] - crop_shape[1]\n",
    "        width_diff = arr.shape[2] - crop_shape[2]\n",
    "\n",
    "        if depth_diff % 2 == 0:\n",
    "            depth_crop_start = depth_diff // 2\n",
    "            depth_crop_end = arr.shape[0] - (depth_diff // 2)\n",
    "        else:\n",
    "            depth_crop_start = depth_diff // 2\n",
    "            depth_crop_end = arr.shape[0] - (depth_diff // 2) - 1\n",
    "\n",
    "        if height_diff % 2 == 0:\n",
    "            height_crop_start = height_diff // 2\n",
    "            height_crop_end = arr.shape[1] - (height_diff // 2)\n",
    "        else:\n",
    "            height_crop_start = height_diff // 2\n",
    "            height_crop_end = arr.shape[1] - (height_diff // 2) - 1\n",
    "\n",
    "        if width_diff % 2 == 0:\n",
    "            width_crop_start = width_diff // 2\n",
    "            width_crop_end = arr.shape[2] - (width_diff // 2)\n",
    "        else:\n",
    "            width_crop_start = width_diff // 2\n",
    "            width_crop_end = arr.shape[2] - (width_diff // 2) - 1\n",
    "\n",
    "        cropped_arr = arr[depth_crop_start:depth_crop_end,\n",
    "                          height_crop_start:height_crop_end, width_crop_start:width_crop_end]\n",
    "\n",
    "        return cropped_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prediction_from_logits(pred):\n",
    "    \n",
    "#     # Create an empty mask with the same size as the prediction\n",
    "#     mask = np.zeros_like(pred[0], dtype=np.uint8)\n",
    "    \n",
    "#     # Print the shape of pred\n",
    "#     print(pred.shape)\n",
    "\n",
    "#     # Count the number of True values for each class\n",
    "#     print(\"Number of True values in class 0:\", np.count_nonzero(pred[0]))\n",
    "#     print(\"Number of True values in class 1:\", np.count_nonzero(pred[1]))\n",
    "#     print(\"Number of True values in class 2:\", np.count_nonzero(pred[2]))\n",
    "\n",
    "#     mask[(pred[0] == True) | (pred[1] == True) | (pred[2] == True)] = 1\n",
    "#     print(\"class 1 mask\")\n",
    "#     print(np.count_nonzero(mask))\n",
    "\n",
    "#     # Tumor Core (TC) is where either the 1st or 3rd prediction channels are 1 (or both)\n",
    "#     mask[(pred[0] == True) | (pred[2] == True)] = 2\n",
    "#     print(np.count_nonzero(mask))\n",
    "#     # Enhancing Tumor (ET) is where the 3rd prediction channel is 1\n",
    "#     mask[pred[2] == True] = 3\n",
    "    \n",
    "#     print(\"Unique values in the resulting mask:\", np.unique(mask))\n",
    "\n",
    "    \n",
    "#     return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter, binary_fill_holes, binary_dilation\n",
    "from skimage.measure import label, regionprops\n",
    "import cc3d\n",
    "from scipy.ndimage.morphology import binary_dilation, binary_fill_holes, binary_closing\n",
    "# def postprocess_prediction(pred):\n",
    "#     # Step 1: Noise Removal\n",
    "#     pred = median_filter(pred, size=3)\n",
    "    \n",
    "#     # Step 2: Dilation\n",
    "#     pred = binary_dilation(pred, structure=np.ones((3, 3, 3)))\n",
    "#     pred = cc3d.connected_components(pred)\n",
    "    \n",
    "#     # Step 3: Fill in the small holes\n",
    "#     pred = binary_fill_holes(pred)\n",
    "    \n",
    "#     # Step 4: Separate Merged Lesion and threshold based on voxel count\n",
    "#     pred_labels = label(pred)\n",
    "#     num_labels = len(np.unique(pred_labels)) - 1  # Subtract 1 for background label\n",
    "#     for region in regionprops(pred_labels):\n",
    "#         if region.area < 50:\n",
    "#             for coords in region.coords:\n",
    "#                 pred_labels[coords[0], coords[1], coords[2]] = 0\n",
    "                \n",
    "#     return pred_labels\n",
    "\n",
    "\n",
    "\n",
    "# def postprocess_prediction(pred, dilation_size=3, min_size=50):\n",
    "#     # Dilation\n",
    "#     pred = binary_dilation(pred, structure=np.ones((dilation_size, dilation_size, dilation_size)))\n",
    "\n",
    "#     # Connected components\n",
    "#     pred = cc3d.connected_components(pred)\n",
    "\n",
    "#     # Size filtering\n",
    "#     unique, counts = np.unique(pred, return_counts=True)\n",
    "#     for u, c in zip(unique, counts):\n",
    "#         if c < min_size:\n",
    "#             pred[pred == u] = 0\n",
    "\n",
    "#     # Closing\n",
    "#     # pred = binary_closing(pred, structure=np.ones((dilation_size, dilation_size, dilation_size)))\n",
    "\n",
    "#     # Hole filling\n",
    "#     # pred = binary_fill_holes(pred)\n",
    "\n",
    "#     return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_from_logits(pred):\n",
    "    # Create an empty mask with the same size as the prediction\n",
    "    mask = np.zeros_like(pred[0], dtype=np.uint8)\n",
    "    # print(pred.shape)\n",
    "    # print(np.count_nonzero(pred))\n",
    "    # print(type(pred))\n",
    "    \n",
    "#     for i in range(pred.shape[0]):\n",
    "#         pred[i] = postprocess_prediction(pred[i])\n",
    "    \n",
    "    \n",
    "    # Enforce class hierarchy: if a voxel is ET, it's also TC and WT\n",
    "    # pred[1] = np.logical_or(pred[1], pred[2]) \n",
    "    # pred[0] = np.logical_or(pred[0], pred[1]) \n",
    "\n",
    "    # Assign the value 2 to the voxels that are in pred[0] but not in pred[1] and pred[2]\n",
    "    mask[(pred[0] == True) & (pred[1] == False) & (pred[2] == False)] = 2\n",
    "\n",
    "    # Assign the value 1 to the voxels that are in pred[0] and pred[1] but not in pred[2]\n",
    "    mask[(pred[0] == True) & (pred[1] == True) & (pred[2] == False)] = 1\n",
    "\n",
    "    # Assign the value 3 to the voxels that are in pred[2]\n",
    "    mask[(pred[2] == True)] = 3\n",
    "    \n",
    "    \n",
    "#     print(\"class 1 mask\")\n",
    "#     print(np.count_nonzero(mask))\n",
    "\n",
    "#     # Tumor Core (TC) is where either the 1st or 3rd prediction channels are 1 (or both)\n",
    "    \n",
    "#     print(np.count_nonzero(mask))\n",
    "#     # Enhancing Tumor (ET) is where the 3rd prediction channel is 1\n",
    "    \n",
    "    print(\"Unique values in the resulting mask:\", np.unique(mask))\n",
    "    print(\"count of 1's in pred[0]\", np.count_nonzero(pred[0]))\n",
    "    print(\"count of 1's in pred[1]\", np.count_nonzero(pred[1]))\n",
    "    print(\"count of 1's in pred[2]\", np.count_nonzero(pred[2]))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.ndimage import binary_dilation, binary_fill_holes\n",
    "# from skimage.measure import label, regionprops\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.ndimage import binary_dilation, binary_erosion\n",
    "# from skimage.measure import label, regionprops\n",
    "# from skimage.morphology import closing, reconstruction\n",
    "\n",
    "# def post_process_ensemble_prediction(pred, dilation_size=3, min_voxel_volume=50, max_hole_volume=10):\n",
    "#     # Get unique labels in the prediction mask\n",
    "#     unique_labels = np.unique(pred)\n",
    "\n",
    "#     # Initiate an array to hold the post-processed prediction\n",
    "#     pred_processed = np.zeros_like(pred)\n",
    "\n",
    "#     for lbl in unique_labels:\n",
    "#         if lbl == 0:\n",
    "#             continue  # Skip background\n",
    "\n",
    "#         # Create a mask for the current label\n",
    "#         label_mask = (pred == lbl)\n",
    "\n",
    "#         # Apply size filtering based on voxel volume\n",
    "#         labeled_mask = label(label_mask)\n",
    "#         regions = regionprops(labeled_mask)\n",
    "#         for region in regions:\n",
    "#             if region.area < min_voxel_volume:  # remove regions smaller than the min_voxel_volume\n",
    "#                 for coordinates in region.coords:\n",
    "#                     labeled_mask[coordinates[0], coordinates[1], coordinates[2]] = 0\n",
    "\n",
    "#         # Apply dilation on the filtered mask\n",
    "#         # dilated_mask = binary_dilation(labeled_mask, structure=np.ones((dilation_size, dilation_size, dilation_size)))\n",
    "\n",
    "#         # If current label is 1, perform hole filling\n",
    "#         if lbl == 1:\n",
    "#             # Perform closing to fill small holes\n",
    "#             closed_mask = closing(dilated_mask, selem=np.ones((dilation_size, dilation_size, dilation_size)))\n",
    "\n",
    "#             # Perform morphological reconstruction to fill all the small holes\n",
    "#             seed = np.copy(closed_mask)\n",
    "#             seed[1:-1, 1:-1, 1:-1] = closed_mask.max()\n",
    "#             reconstructed_mask = reconstruction(seed, closed_mask, method='erosion')\n",
    "\n",
    "#             # The resulting 'reconstructed_mask' is a float image, due to the reconstruction process. Let's convert it to binary\n",
    "#             final_mask = (reconstructed_mask > 0.5).astype(int)\n",
    "\n",
    "#             dilated_mask = final_mask\n",
    "\n",
    "#         # Add the processed mask for the current label to the final prediction\n",
    "#         pred_processed[dilated_mask > 0] = lbl\n",
    "\n",
    "#     return pred_processed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, opening, reconstruction, dilation\n",
    "\n",
    "def post_process_ensemble_prediction(pred, closing_size=5, opening_size=3, min_voxel_volume=50, dilation_size=3):\n",
    "    # Get unique labels in the prediction mask\n",
    "    unique_labels = np.unique(pred)\n",
    "\n",
    "    # Initiate an array to hold the post-processed prediction\n",
    "    pred_processed = np.zeros_like(pred)\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        if lbl == 0:\n",
    "            continue  # Skip background\n",
    "\n",
    "        # Create a mask for the current label\n",
    "        label_mask = (pred == lbl)\n",
    "\n",
    "        # Apply size filtering based on voxel volume\n",
    "        labeled_mask = label(label_mask)\n",
    "        regions = regionprops(labeled_mask)\n",
    "        for region in regions:\n",
    "            if region.area < min_voxel_volume:  # remove regions smaller than the min_voxel_volume\n",
    "                for coordinates in region.coords:\n",
    "                    labeled_mask[coordinates[0], coordinates[1], coordinates[2]] = 0\n",
    "                    # If the region is small, apply dilation\n",
    "\n",
    "        # If current label is 1, perform hole filling\n",
    "        if lbl == 1:\n",
    "            # Perform closing to fill small holes\n",
    "            labeled_mask = closing(labeled_mask, selem=np.ones((closing_size, closing_size, closing_size)))\n",
    "\n",
    "            # Perform morphological reconstruction to fill all the small holes\n",
    "        seed = np.copy(labeled_mask)\n",
    "        seed[1:-1, 1:-1, 1:-1] = labeled_mask.max()\n",
    "        reconstructed_mask = reconstruction(seed, labeled_mask, method='erosion')\n",
    "\n",
    "        # The resulting 'reconstructed_mask' is a float image, due to the reconstruction process. Let's convert it to binary\n",
    "        final_mask = (reconstructed_mask > 0.5).astype(int)\n",
    "\n",
    "        # Replace labeled_mask with the final mask (which has filled holes) for label 1\n",
    "        labeled_mask = final_mask\n",
    "\n",
    "        # Add the processed mask for the current label to the final prediction\n",
    "        pred_processed[labeled_mask > 0] = lbl\n",
    "\n",
    "    return pred_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model1, model2, model3, model4, model5, model_ET, model_TC, model_WT):\n",
    "    dataset = OwnDataset()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    models = [ model1, model2, model3,model4,model5]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            path = dataset.images[i][0]\n",
    "            data = data.to(device)\n",
    "\n",
    "            # List to hold all logits for each model\n",
    "            logits_list = []\n",
    "\n",
    "            for model in models:\n",
    "                logits = model(data)\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                logits = logits.squeeze(0)\n",
    "                logits = np.moveaxis(logits,  (0, 3, 2, 1),(0, 1, 2, 3))\n",
    "                logits_list.append(logits)\n",
    "\n",
    "                           # Get averaged logits for each channel\n",
    "            average_logits = np.zeros_like(logits_list[0])\n",
    "            # for channel in range(logits_list[0].shape[0]):\n",
    "            #     summed_logits = np.sum(np.array([logit[channel] for logit in logits_list]), axis=0)\n",
    "            #     average_logits[channel] = summed_logits / len(models)  # Calculate the average\n",
    "            # print(average_logits)\n",
    "\n",
    "            logits_ET = model_ET(data).detach().cpu().numpy()\n",
    "            logits_ET = logits_ET.squeeze(0)\n",
    "            logits_ET = np.moveaxis(logits_ET,  (0, 3, 2, 1),(0, 1, 2, 3))\n",
    "\n",
    "            logits_TC = model_TC(data).detach().cpu().numpy()\n",
    "            logits_TC = logits_TC.squeeze(0)\n",
    "            logits_TC = np.moveaxis(logits_TC,  (0, 3, 2, 1),(0, 1, 2, 3))\n",
    "\n",
    "            logits_WT = model_WT(data).detach().cpu().numpy()\n",
    "            logits_WT = logits_WT.squeeze(0)\n",
    "            logits_WT = np.moveaxis(logits_WT,  (0, 3, 2, 1),(0, 1, 2, 3))\n",
    "\n",
    "            # Threshold the individual model predictions\n",
    "            logits_ET = (logits_ET > 0.53).astype(int)\n",
    "            logits_TC = (logits_TC > 0.53).astype(int)\n",
    "            logits_WT = (logits_WT > 0.53).astype(int)\n",
    "\n",
    "            # majority_vote = np.zeros_like(average_logits[0])\n",
    "\n",
    "            for channel in range(logits_list[0].shape[0]):\n",
    "                summed_logits = np.sum(np.array([logit[channel] for logit in logits_list]), axis=0)\n",
    "                average_logits[channel] = np.where(summed_logits > 4, 1, 0)\n",
    "                \n",
    "\n",
    "                # Threshold the averaged logits\n",
    "            # average_logits[0] = (average_logits[0] > 0.53).astype(int)\n",
    "            # average_logits[1] = (average_logits[1] > 0.53).astype(int)\n",
    "            # average_logits[2] = (average_logits[2] > 0.53).astype(int)\n",
    "\n",
    "\n",
    "            # print(np.unique(logits_ET),np.unique(logits_WT[0]))\n",
    "\n",
    "            # Apply condition for ET and WT\n",
    "            # average_logits[0] = np.where((average_logits[0] == 1) & (logits_WT == 1), 1, 0)\n",
    "            # average_logits[1] = np.where((average_logits[1] == 1) & (logits_TC == 1), 1, 0)\n",
    "            # average_logits[2] = np.where((average_logits[2]==1) | (logits_ET == 1), 1, 0)\n",
    "                \n",
    "                \n",
    "                \n",
    "            predictions = get_prediction_from_logits(average_logits)\n",
    "            # print(predictions.shape)\n",
    "            \n",
    "            \n",
    "\n",
    "            # predictions = postprocess_prediction(average_logits)\n",
    "            # print(np.unique(predictions))\n",
    "\n",
    "            # Apply post-processing techniques\n",
    "            predictions = post_process_ensemble_prediction(predictions)\n",
    "            # predictions = postprocess_prediction(predictions)\n",
    "\n",
    "            \n",
    "            \n",
    "            brain_3D_nib = nib.load(path)\n",
    "\n",
    "            pred_mask_3D_nib = nib.Nifti1Image(predictions, affine = brain_3D_nib.affine, header = brain_3D_nib.header)\n",
    "            nib.save(pred_mask_3D_nib, os.path.join(\"outputs_folder/combine_8_models_d1\", os.path.basename(path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_prediction(model):\n",
    "#     dataset = OwnDataset()\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(dataset)):\n",
    "#             data = dataset[i]\n",
    "#             path = dataset.images[i][0]\n",
    "#             data = data.to(device)\n",
    "#             print(data.shape)\n",
    "#             logits = model(data)\n",
    "#             print(logits.shape)\n",
    "#             logits = logits.detach().cpu().numpy() > 0.50\n",
    "#             print(\"logits shape\")\n",
    "                  \n",
    "#             # print(logits.shape)\n",
    "            \n",
    "#             logits = logits.squeeze(0)\n",
    "#             logits = np.moveaxis(logits,  (0, 3, 2, 1),(0, 1, 2, 3))\n",
    "            \n",
    "#             print(logits.shape)\n",
    "#             # predictions = np.moveaxis(predictions, (0, 1, 2), (2, 1, 0))\n",
    "#             predictions = get_prediction_from_logits(logits)\n",
    "#             print(predictions.shape)\n",
    "            \n",
    "#             print(np.unique(predictions))\n",
    "            \n",
    "#             brain_3D_nib = nib.load(path)\n",
    "\n",
    "#             pred_mask_3D_nib = nib.Nifti1Image(predictions, affine = brain_3D_nib.affine, header = brain_3D_nib.header)\n",
    "#             nib.save(pred_mask_3D_nib, os.path.join(\"outputs_folder/output_ONet_post\", os.path.basename(path)))\n",
    "            \n",
    "#             pred_mask_3D_nib = nib.Nifti1Image(predictions[1], affine = brain_3D_nib.affine, header = brain_3D_nib.header)\n",
    "#             nib.save(pred_mask_3D_nib, \"2new__\" + os.path.basename(path))\n",
    "            \n",
    "#             pred_mask_3D_nib = nib.Nifti1Image(predictions[2], affine = brain_3D_nib.affine, header = brain_3D_nib.header)\n",
    "#             nib.save(pred_mask_3D_nib, \"3new__\" + os.path.basename(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_prediction(model1, model2):\n",
    "#     dataset = OwnDataset()\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     models = [model1, model2]\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(dataset)):\n",
    "#             data = dataset[i]\n",
    "#             path = dataset.images[i][0]\n",
    "#             data = data.to(device)\n",
    "\n",
    "#             # List to hold all logits for each model\n",
    "#             logits_list = []\n",
    "\n",
    "#             for model in models:\n",
    "#                 logits = model(data)\n",
    "#                 logits = logits.detach().cpu().numpy() > 0.53\n",
    "#                 logits = logits.squeeze(0)\n",
    "#                 logits = np.moveaxis(logits,  (0, 3, 2, 1),(0, 1, 2, 3))\n",
    "#                 logits_list.append(logits)\n",
    "\n",
    "#             # Perform majority voting for each channel\n",
    "#             majority_vote = np.zeros_like(logits_list[0])\n",
    "\n",
    "#             for channel in range(logits_list[0].shape[0]):\n",
    "#                 summed_logits = np.sum(np.array([logit[channel] for logit in logits_list]), axis=0)\n",
    "#                 majority_vote[channel] = np.where(summed_logits > len(models) / 2, 1, 0)\n",
    "\n",
    "#             predictions = get_prediction_from_logits(majority_vote)\n",
    "\n",
    "#             print(np.unique(predictions))\n",
    "\n",
    "#             brain_3D_nib = nib.load(path)\n",
    "\n",
    "#             pred_mask_3D_nib = nib.Nifti1Image(predictions, affine = brain_3D_nib.affine, header = brain_3D_nib.header)\n",
    "#             nib.save(pred_mask_3D_nib, os.path.join(\"outputs_folder/combine_2_final_tr_Unet_Onet_del\", os.path.basename(path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary which maps the model names/directories in the Logs folder to their respective model initialisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDict = {\n",
    "    \"3DOnet_DoubleConv_Kernel1\": ONet3d_v3_DoubleConv(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1\": ONet3d_v3(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1_32_Channels\": ONet3d_v3(in_channels=4, n_classes=3, n_channels=32).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1_GELU\": ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel1_GELU_AdamW\": ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel3\": ONet3d_v2(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DOnet_SingleConv_Kernel5\": ONet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet\": UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_32_Channels\": UNet3d(in_channels=4, n_classes=3, n_channels=32).to('cuda'),\n",
    "    \"3DUnet_Atten\": UNet3d_atten(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_Dropout\": UNet3dDropout(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_GELU\": UNet3d_GELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_SELU\": UNet3d_SELU(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"3DUnet_SingleConv\": UNet3dSingleConv(in_channels=4, n_classes=3, n_channels=24).to('cuda'),\n",
    "    \"SphereNet3D\": SphereNet3d(in_channels=4, n_classes=3, n_channels=16).to('cuda'),\n",
    "    \"SwinUNETR\": SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(1, 1, 1, 1), num_heads=(2, 4, 8, 16)).to('cuda'),\n",
    "    \"SwinUNETR_AdamW\": SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(1, 1, 1, 1), num_heads=(2, 4, 8, 16)).to('cuda'),\n",
    "    \"SwinUNETR_DoubleLayerDepth\": SwinUNETR(in_channels=4, out_channels=3, img_size=(128, 224, 224), depths=(2, 2, 2, 2), num_heads=(2, 4, 8, 16)).to('cuda'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data.csv\")\n",
    "df2 = pd.read_csv(\"test_data.csv\")\n",
    "combined_df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Write the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('combined_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function which takes in a numerical user input and outputs a string representation of the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseModel():\n",
    "    availableActions = {str(i+1): k for (i, k)\n",
    "                        in zip(range(len(modelDict)), modelDict.keys())}\n",
    "    nl = '\\n'\n",
    "    # Takes in a dictionary with key/value pair corresponding with control/action\n",
    "    availableActionsList = [(key, val)\n",
    "                            for key, val in availableActions.items()]\n",
    "    print(f\"Use number keys to choose one of the models below: \\n\")\n",
    "    print(\n",
    "        f\"Available Models: {nl.join(f'[{tup[0]}: {tup[1]}]' for tup in availableActionsList)}\")\n",
    "    sleep(1)\n",
    "    while True:\n",
    "        userInput = input(\"Enter your action: \")\n",
    "        if userInput not in availableActions:\n",
    "            print(\n",
    "                f\"{userInput} is an invalid action. Please try again.\")\n",
    "        else:\n",
    "            break\n",
    "    return availableActions[userInput]\n",
    "\n",
    "\n",
    "# chooseModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run this cell to Evaluate and visualise predictions for a single model. Model is selected by inputting a number when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    path_to_csv: str,\n",
    "    # phase: str,\n",
    "    val_fold: int = 1,  # Choose which fold to be the validation fold\n",
    "    test_fold: int = 0,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 4,\n",
    "    do_resizing: bool = True,\n",
    "):\n",
    "    assert (val_fold != test_fold)\n",
    "\n",
    "    df = pd.read_csv(\"combined_file.csv\")\n",
    "\n",
    "    '''Returns: dataloader for the model training'''\n",
    "    # Data in folds other than 0 are used for training\n",
    "    train_df = df.loc[~df['fold'].isin(\n",
    "        [val_fold, test_fold])].reset_index(drop=True)\n",
    "    # Data in fold 0 is used for validation\n",
    "    val_df = df.loc[df['fold'] == val_fold].reset_index(drop=True)\n",
    "    test_df = df.loc[df['fold'] == test_fold].reset_index(drop=True)\n",
    "\n",
    "    # dataset = dataset(df, phase)\n",
    "    train_dataset = dataset(train_df, \"train\", do_resizing=do_resizing)\n",
    "    val_dataset = dataset(val_df, \"val\", do_resizing=do_resizing)\n",
    "    test_dataset = dataset(test_df, \"test\", do_resizing=do_resizing)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations import Compose\n",
    "import os\n",
    "\n",
    "\n",
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, df: pd.read_csv(\"combined_file.csv\"), phase: str = \"test\", do_resizing: bool = False):\n",
    "        # Dataframe containing patient, path and fold mapping information\n",
    "        self.df = df\n",
    "        # \"train\" \"valid\" or \"test\". Determines whether to apply preprocessing\n",
    "        self.phase = phase\n",
    "        self.augmentations = self.get_augmentations(phase)\n",
    "        self.data_types = ['-t1c.nii.gz', '-t1n.nii.gz', '-t2w.nii.gz', '-t2f.nii.gz']\n",
    "        self.do_resizing = do_resizing\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    # Makes class accessible by square-bracket notations; determines behaviour upon square-bracket access\n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.df.loc[idx, 'Brats20ID']\n",
    "        root_path = self.df.loc[self.df['Brats20ID'] == id_]['path'].values[0]\n",
    "        images = []\n",
    "        for data_type in self.data_types:\n",
    "            img_path = os.path.join(root_path, 'BraTS-PED-' + id_.split('_')[-1].zfill(5) + '-000' + data_type)\n",
    "            img = self.load_img(img_path)\n",
    "            if self.do_resizing:\n",
    "                img = self.resize(img)\n",
    "            img = self.normalize(img)\n",
    "            images.append(img)\n",
    "        img = np.stack(images)\n",
    "        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "        if self.phase == 'test':\n",
    "            return {\n",
    "                \"Id\": id_,\n",
    "                \"image\": img\n",
    "            }\n",
    "    # TO-DO: Implement possible augmentations here? Lower priority for now\n",
    "\n",
    "    def get_augmentations(self, phase):\n",
    "        list_transforms = []\n",
    "        list_trfms = Compose(list_transforms,is_check_shapes=False )\n",
    "        return list_trfms\n",
    "\n",
    "    def load_img(self, file_path):\n",
    "        data = nib.load(file_path)\n",
    "        data = np.asarray(data.dataobj)\n",
    "        return data\n",
    "\n",
    "    def normalize(self, data: np.ndarray):\n",
    "        data_min = np.min(data)\n",
    "        return (data - data_min) / (np.max(data) - data_min)\n",
    "\n",
    "    def resize(self, data: np.ndarray):\n",
    "        # data = resize(data, (224, 224, 128), preserve_range=True)\n",
    "        data = self.crop_3d_array(data, (240, 240, 155))\n",
    "        return data\n",
    "\n",
    "    def preprocess_mask_labels(self, mask: np.ndarray):\n",
    "        # In the BraTS challenge, the segmentation performance is evaluated on three partially overlapping sub-regions of tumors,\n",
    "        # namely, whole tumor (WT), tumor core (TC), and enhancing tumor (ET).\n",
    "        # The WT is the union of ED, NCR/NET, and ET, while the TC includes NCR/NET and ET.\n",
    "\n",
    "        mask_WT = mask.copy()\n",
    "        mask_WT[mask_WT == 1] = 1\n",
    "        mask_WT[mask_WT == 2] = 1\n",
    "        mask_WT[mask_WT == 3] = 1\n",
    "\n",
    "        mask_TC = mask.copy()\n",
    "        mask_TC[mask_TC == 1] = 1\n",
    "        mask_TC[mask_TC == 2] = 0\n",
    "        mask_TC[mask_TC == 3] = 1\n",
    "\n",
    "        mask_ET = mask.copy()\n",
    "        mask_ET[mask_ET == 1] = 0\n",
    "        mask_ET[mask_ET == 2] = 0\n",
    "        mask_ET[mask_ET == 3] = 1\n",
    "\n",
    "        mask = np.stack([mask_WT, mask_TC, mask_ET])\n",
    "        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def crop_3d_array(self, arr, crop_shape):\n",
    "        \"\"\"\n",
    "        Crop a 3D array to the specified shape.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : numpy.ndarray\n",
    "            The 3D input array to be cropped.\n",
    "        crop_shape : tuple\n",
    "            The shape of the cropped array. Must be a 3-element tuple (depth, height, width).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The cropped array.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(crop_shape) == 3, \"crop_shape must be a 3-element tuple\"\n",
    "        assert crop_shape[0] <= arr.shape[0], \"depth of crop_shape must be <= depth of arr\"\n",
    "        assert crop_shape[1] <= arr.shape[1], \"height of crop_shape must be <= height of arr\"\n",
    "        assert crop_shape[2] <= arr.shape[2], \"width of crop_shape must be <= width of arr\"\n",
    "\n",
    "        depth_diff = arr.shape[0] - crop_shape[0]\n",
    "        height_diff = arr.shape[1] - crop_shape[1]\n",
    "        width_diff = arr.shape[2] - crop_shape[2]\n",
    "\n",
    "        if depth_diff % 2 == 0:\n",
    "            depth_crop_start = depth_diff // 2\n",
    "            depth_crop_end = arr.shape[0] - (depth_diff // 2)\n",
    "        else:\n",
    "            depth_crop_start = depth_diff // 2\n",
    "            depth_crop_end = arr.shape[0] - (depth_diff // 2) - 1\n",
    "\n",
    "        if height_diff % 2 == 0:\n",
    "            height_crop_start = height_diff // 2\n",
    "            height_crop_end = arr.shape[1] - (height_diff // 2)\n",
    "        else:\n",
    "            height_crop_start = height_diff // 2\n",
    "            height_crop_end = arr.shape[1] - (height_diff // 2) - 1\n",
    "\n",
    "        if width_diff % 2 == 0:\n",
    "            width_crop_start = width_diff // 2\n",
    "            width_crop_end = arr.shape[2] - (width_diff // 2)\n",
    "        else:\n",
    "            width_crop_start = width_diff // 2\n",
    "            width_crop_end = arr.shape[2] - (width_diff // 2) - 1\n",
    "\n",
    "        cropped_arr = arr[depth_crop_start:depth_crop_end,\n",
    "                          height_crop_start:height_crop_end, width_crop_start:width_crop_end]\n",
    "\n",
    "        return cropped_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalSingle():\n",
    "    # model_name = chooseModel()\n",
    "    # print(f\"'{model_name}' selected for evaluation and visualisation\")\n",
    "\n",
    "    model_name = \"3DUnet_GELU\"\n",
    "    # model = ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    # model2 = SphereNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # model1 = ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=64).to('cuda')\n",
    "    # model2 = ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    # model3 = UNet3d_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    # model4 = UNet3d_GELU(in_channels=4, n_classes=3, n_channels=64).to('cuda')\n",
    "    \n",
    "    model_1 = ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    model_2 = UNet3d_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    model_3 = ONet3d_v2(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    model_4 = ONet3d_v3_DoubleConv(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    model_5 = ONet3d(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_ET = ONet3d_v3_GELU(in_channels=4, n_classes=1, n_channels=32).to('cuda')\n",
    "    model_WT = ONet3d_v3_GELU(in_channels=4, n_classes=1, n_channels=32).to('cuda')    \n",
    "    model_TC = ONet3d_v3_GELU(in_channels=4, n_classes=1, n_channels=32).to('cuda')    \n",
    "    \n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    checkpoint_path = None\n",
    "    \n",
    "    \n",
    "    # checkpoint_path = \"Logs/3DUnet_your_modifications/your_last_epoch_model_20230726-122634.pth\"\n",
    "    # checkpoint_path_2 = \"Logs/3DUnet_your_modifications/your_best_model_20230727-032820_96_Sphere_24c.pth  \n",
    "    \n",
    "\n",
    "    checkpoint_path_1 = \"Logs/ONet_32_31_07/your_best_model_20230801-144818_199_Onet_d31_c32.pth\"\n",
    "    checkpoint_path_2  = \"Logs/UNet_32_31_07/your_best_model_20230801-114142_199_UNet_d31_c32.pth\"\n",
    "    checkpoint_path_3 = \"Logs/ONet3d_v2_32_31_07/your_best_model_20230801-040449_199_Onet_v2_d31_c32.pth\"\n",
    "    checkpoint_path_4 = \"Logs/ONet_32_DoubleConv_31_07/your_best_model_20230801-045347_199_Onet_32_DoubleConv_d31_c32.pth\"\n",
    "    checkpoint_path_5 = \"Logs/only_ONet_32_31_07/your_best_model_20230801-071616_199_only_Onet_d31_c32.pth\"\n",
    "    \n",
    "    checkpoint_path_ET = \"Logs/ONet_32_31_07_ET/your_best_model_20230801-024903_199_Onet_d31_c32_ET.pth\"\n",
    "    checkpoint_path_TC = \"Logs/ONet_32_31_07_TC/your_best_model_20230801-150513_155_Onet_d31_c32_TC.pth\"\n",
    "    checkpoint_path_WT = \"Logs/3DUnet_your_modifications/your_last_epoch_model_20230728-020546.pth\"\n",
    "       \n",
    "      # break\n",
    "        \n",
    "# work/pi_gschlaug_umass_edu/Shashi_files/3D_exp_folder/3D_Brain_Tumor_Seg_V2-master/Logs/3DUnet_your_modifications/your_best_model_20230719-221836.pth\n",
    "    # checkpoint_path = \"Logs/3DUnet_your_modifications/your_best_model_20230719-155138.pth\"\n",
    "\n",
    "    try:\n",
    "        # model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "        # model.eval()\n",
    "        model_1.load_state_dict(torch.load(checkpoint_path_1, map_location='cpu'))\n",
    "        model_1.eval()\n",
    "        model_2.load_state_dict(torch.load(checkpoint_path_2, map_location='cpu'))\n",
    "        model_2.eval()\n",
    "        model_3.load_state_dict(torch.load(checkpoint_path_3, map_location='cpu'))\n",
    "        model_3.eval()\n",
    "        model_4.load_state_dict(torch.load(checkpoint_path_4, map_location='cpu'))\n",
    "        model_4.eval()\n",
    "        model_5.load_state_dict(torch.load(checkpoint_path_5, map_location='cpu'))\n",
    "        model_5.eval()\n",
    "        model_ET.load_state_dict(torch.load(checkpoint_path_ET, map_location='cpu'))\n",
    "        model_ET.eval()\n",
    "        model_TC.load_state_dict(torch.load(checkpoint_path_TC, map_location='cpu'))\n",
    "        model_TC.eval()\n",
    "        model_WT.load_state_dict(torch.load(checkpoint_path_WT, map_location='cpu'))\n",
    "        model_WT.eval()\n",
    "        \n",
    "        \n",
    "        # print(f\"{model_name} loaded with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error loading {model_name} with chkpt: {checkpoint_path}. parameters: {count_parameters(model)}\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    generate_prediction(model_1, model_2, model_3, model_4, model_5, model_ET, model_TC, model_WT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 182720\n",
      "count of 1's in pred[1] 105687\n",
      "count of 1's in pred[2] 42949\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 11199\n",
      "count of 1's in pred[1] 8194\n",
      "count of 1's in pred[2] 59\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 79980\n",
      "count of 1's in pred[1] 78168\n",
      "count of 1's in pred[2] 22893\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 50967\n",
      "count of 1's in pred[1] 49705\n",
      "count of 1's in pred[2] 496\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 220381\n",
      "count of 1's in pred[1] 134849\n",
      "count of 1's in pred[2] 3411\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 48886\n",
      "count of 1's in pred[1] 48673\n",
      "count of 1's in pred[2] 993\n",
      "Unique values in the resulting mask: [0 1 2]\n",
      "count of 1's in pred[0] 23868\n",
      "count of 1's in pred[1] 23743\n",
      "count of 1's in pred[2] 0\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 11812\n",
      "count of 1's in pred[1] 11690\n",
      "count of 1's in pred[2] 3924\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 36590\n",
      "count of 1's in pred[1] 36483\n",
      "count of 1's in pred[2] 6158\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 49061\n",
      "count of 1's in pred[1] 48786\n",
      "count of 1's in pred[2] 192\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 12597\n",
      "count of 1's in pred[1] 12487\n",
      "count of 1's in pred[2] 1\n",
      "Unique values in the resulting mask: [0 1 2]\n",
      "count of 1's in pred[0] 58193\n",
      "count of 1's in pred[1] 57749\n",
      "count of 1's in pred[2] 0\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 24080\n",
      "count of 1's in pred[1] 23774\n",
      "count of 1's in pred[2] 8862\n",
      "Unique values in the resulting mask: [0 1 2]\n",
      "count of 1's in pred[0] 36041\n",
      "count of 1's in pred[1] 35883\n",
      "count of 1's in pred[2] 0\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 21526\n",
      "count of 1's in pred[1] 21350\n",
      "count of 1's in pred[2] 159\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 37676\n",
      "count of 1's in pred[1] 37451\n",
      "count of 1's in pred[2] 6567\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 13532\n",
      "count of 1's in pred[1] 13455\n",
      "count of 1's in pred[2] 206\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 76797\n",
      "count of 1's in pred[1] 54065\n",
      "count of 1's in pred[2] 2397\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 37273\n",
      "count of 1's in pred[1] 27303\n",
      "count of 1's in pred[2] 590\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 13401\n",
      "count of 1's in pred[1] 7951\n",
      "count of 1's in pred[2] 496\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 47101\n",
      "count of 1's in pred[1] 46721\n",
      "count of 1's in pred[2] 27425\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 129896\n",
      "count of 1's in pred[1] 72760\n",
      "count of 1's in pred[2] 359\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 60059\n",
      "count of 1's in pred[1] 37697\n",
      "count of 1's in pred[2] 6254\n",
      "Unique values in the resulting mask: [0 1 2]\n",
      "count of 1's in pred[0] 14946\n",
      "count of 1's in pred[1] 10108\n",
      "count of 1's in pred[2] 0\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 110203\n",
      "count of 1's in pred[1] 84501\n",
      "count of 1's in pred[2] 1605\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 26968\n",
      "count of 1's in pred[1] 17111\n",
      "count of 1's in pred[2] 7764\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 123754\n",
      "count of 1's in pred[1] 65404\n",
      "count of 1's in pred[2] 17490\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 16562\n",
      "count of 1's in pred[1] 10791\n",
      "count of 1's in pred[2] 8596\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 35974\n",
      "count of 1's in pred[1] 29991\n",
      "count of 1's in pred[2] 682\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 141533\n",
      "count of 1's in pred[1] 95262\n",
      "count of 1's in pred[2] 37593\n",
      "Unique values in the resulting mask: [0 1 2]\n",
      "count of 1's in pred[0] 41963\n",
      "count of 1's in pred[1] 21830\n",
      "count of 1's in pred[2] 0\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 2040\n",
      "count of 1's in pred[1] 1751\n",
      "count of 1's in pred[2] 972\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 94477\n",
      "count of 1's in pred[1] 53723\n",
      "count of 1's in pred[2] 13391\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 65975\n",
      "count of 1's in pred[1] 21675\n",
      "count of 1's in pred[2] 882\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 22501\n",
      "count of 1's in pred[1] 17512\n",
      "count of 1's in pred[2] 6285\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 56114\n",
      "count of 1's in pred[1] 31686\n",
      "count of 1's in pred[2] 2607\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 5031\n",
      "count of 1's in pred[1] 2621\n",
      "count of 1's in pred[2] 624\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 64898\n",
      "count of 1's in pred[1] 21112\n",
      "count of 1's in pred[2] 4\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 73529\n",
      "count of 1's in pred[1] 49496\n",
      "count of 1's in pred[2] 3475\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 35426\n",
      "count of 1's in pred[1] 35133\n",
      "count of 1's in pred[2] 1608\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 141362\n",
      "count of 1's in pred[1] 95555\n",
      "count of 1's in pred[2] 57743\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 105220\n",
      "count of 1's in pred[1] 100603\n",
      "count of 1's in pred[2] 1308\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 45095\n",
      "count of 1's in pred[1] 44899\n",
      "count of 1's in pred[2] 589\n",
      "Unique values in the resulting mask: [0 1 2]\n",
      "count of 1's in pred[0] 33672\n",
      "count of 1's in pred[1] 33504\n",
      "count of 1's in pred[2] 0\n",
      "Unique values in the resulting mask: [0 1 2 3]\n",
      "count of 1's in pred[0] 127813\n",
      "count of 1's in pred[1] 106622\n",
      "count of 1's in pred[2] 132\n"
     ]
    }
   ],
   "source": [
    "evalSingle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display accuracy for all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory tr_pediatric_modalities/t2w/.ipynb_checkpoints has been removed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Specify the directory path\n",
    "dir_path = \"tr_pediatric_modalities/t2w/.ipynb_checkpoints\"\n",
    "\n",
    "# Check if the directory exists before trying to remove it\n",
    "if os.path.exists(dir_path):\n",
    "    # Remove the directory and all its contents\n",
    "    shutil.rmtree(dir_path)\n",
    "    print(f\"Directory {dir_path} has been removed successfully\")\n",
    "else:\n",
    "    print(f\"No such directory: {dir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ids</th>\n",
       "      <th>WT dice</th>\n",
       "      <th>WT jaccard</th>\n",
       "      <th>TC dice</th>\n",
       "      <th>TC jaccard</th>\n",
       "      <th>ET dice</th>\n",
       "      <th>ET jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BraTS20_Training_003</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>0.787001</td>\n",
       "      <td>0.850592</td>\n",
       "      <td>0.740025</td>\n",
       "      <td>0.858111</td>\n",
       "      <td>0.751483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BraTS20_Training_004</td>\n",
       "      <td>0.830250</td>\n",
       "      <td>0.709766</td>\n",
       "      <td>0.860727</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.531713</td>\n",
       "      <td>0.362131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Ids   WT dice  WT jaccard   TC dice  TC jaccard   ET dice  \\\n",
       "0  BraTS20_Training_003  0.880806    0.787001  0.850592    0.740025  0.858111   \n",
       "1  BraTS20_Training_004  0.830250    0.709766  0.860727    0.755505  0.531713   \n",
       "\n",
       "   ET jaccard  \n",
       "0    0.751483  \n",
       "1    0.362131  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or no access: 'ASNR-MICCAI-BraTS2023-PED-Challenge-ValidationData/BraTS-PED-00030-000/BraTS-PED-00030-000-seg.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ASNR-MICCAI-BraTS2023-PED-Challenge-ValidationData/BraTS-PED-00030-000/BraTS-PED-00030-000-seg.nii.gz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_427550/4116788151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Now you can use the model to make predictions on new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_427550/2791127487.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BraTS-PED-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-000'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-seg.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_resizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_427550/2791127487.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mstat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No such file or no access: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstat_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImageFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Empty file: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: 'ASNR-MICCAI-BraTS2023-PED-Challenge-ValidationData/BraTS-PED-00030-000/BraTS-PED-00030-000-seg.nii.gz'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Assuming that the new data is a DataFrame stored in a CSV file\n",
    "new_data_path = \"test_data.csv\"\n",
    "new_data = pd.read_csv(new_data_path)\n",
    "\n",
    "# Initialize dataset with new data\n",
    "new_dataset = BratsDataset(df=new_data, phase=\"test\", do_resizing=False)\n",
    "\n",
    "# Initialize a DataLoader with the new dataset\n",
    "# Note: set batch_size according to your memory capacity\n",
    "new_data_loader = DataLoader(new_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define the classes\n",
    "classes = [\"WT\", \"TC\", \"ET\"]\n",
    "model = ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "\n",
    "# Initialize the model\n",
    "checkpoint_path = \"Logs/3DUnet_your_modifications/your_best_model_20230719-155138.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu')) # replace with your actual model\n",
    "\n",
    "# Load the model weights\n",
    "# model_path = \"path_to_your_saved_model.pth\"\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model = model.to(trainer.device)  # Ensure the model is in the same device as your data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Now you can use the model to make predictions on new data\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(new_data_loader):\n",
    "        imgs = data['image'].to(trainer.device)\n",
    "        logits = model(imgs)\n",
    "\n",
    "        # Convert logits to segmented masks\n",
    "        pred_masks = np.argmax(logits.cpu().numpy(), axis=1)  # change axis accordingly\n",
    "\n",
    "        # Save segmented mask as .nii file\n",
    "        # Assuming you have a corresponding affine for each image\n",
    "        affine = data['affine']\n",
    "        pred_img = nib.Nifti1Image(pred_masks, affine)\n",
    "        nib.save(pred_img, f'prediction_{i}.nii.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations import Compose\n",
    "import os\n",
    "\n",
    "\n",
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, df: pd.read_csv(\"data.csv\"), phase: str = \"test\", do_resizing: bool = False):\n",
    "        # Dataframe containing patient, path and fold mapping information\n",
    "        self.df = df\n",
    "        # \"train\" \"valid\" or \"test\". Determines whether to apply preprocessing\n",
    "        self.phase = phase\n",
    "        self.augmentations = self.get_augmentations(phase)\n",
    "        self.data_types = ['-t1c.nii.gz', '-t1n.nii.gz', '-t2w.nii.gz', '-t2f.nii.gz']\n",
    "        self.do_resizing = do_resizing\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    # Makes class accessible by square-bracket notations; determines behaviour upon square-bracket access\n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.df.loc[idx, 'Brats20ID']\n",
    "        root_path = self.df.loc[self.df['Brats20ID'] == id_]['path'].values[0]\n",
    "        images = []\n",
    "        for data_type in self.data_types:\n",
    "            img_path = os.path.join(root_path, 'BraTS-PED-' + id_.split('_')[-1].zfill(5) + '-000' + data_type)\n",
    "            img = self.load_img(img_path)\n",
    "            if self.do_resizing:\n",
    "                img = self.resize(img)\n",
    "            img = self.normalize(img)\n",
    "            images.append(img)\n",
    "        img = np.stack(images)\n",
    "        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "        if self.phase == 'test':\n",
    "            return {\n",
    "                \"Id\": id_,\n",
    "                \"image\": img\n",
    "            }\n",
    "    # TO-DO: Implement possible augmentations here? Lower priority for now\n",
    "\n",
    "    def get_augmentations(self, phase):\n",
    "        list_transforms = []\n",
    "        list_trfms = Compose(list_transforms,is_check_shapes=False )\n",
    "        return list_trfms\n",
    "\n",
    "    def load_img(self, file_path):\n",
    "        data = nib.load(file_path)\n",
    "        data = np.asarray(data.dataobj)\n",
    "        return data\n",
    "\n",
    "    def normalize(self, data: np.ndarray):\n",
    "        data_min = np.min(data)\n",
    "        return (data - data_min) / (np.max(data) - data_min)\n",
    "\n",
    "    def resize(self, data: np.ndarray):\n",
    "        # data = resize(data, (224, 224, 128), preserve_range=True)\n",
    "        data = self.crop_3d_array(data, (240, 240, 155))\n",
    "        return data\n",
    "\n",
    "    def preprocess_mask_labels(self, mask: np.ndarray):\n",
    "        # In the BraTS challenge, the segmentation performance is evaluated on three partially overlapping sub-regions of tumors,\n",
    "        # namely, whole tumor (WT), tumor core (TC), and enhancing tumor (ET).\n",
    "        # The WT is the union of ED, NCR/NET, and ET, while the TC includes NCR/NET and ET.\n",
    "\n",
    "        mask_WT = mask.copy()\n",
    "        mask_WT[mask_WT == 1] = 1\n",
    "        mask_WT[mask_WT == 2] = 1\n",
    "        mask_WT[mask_WT == 3] = 1\n",
    "\n",
    "        mask_TC = mask.copy()\n",
    "        mask_TC[mask_TC == 1] = 1\n",
    "        mask_TC[mask_TC == 2] = 0\n",
    "        mask_TC[mask_TC == 3] = 1\n",
    "\n",
    "        mask_ET = mask.copy()\n",
    "        mask_ET[mask_ET == 1] = 0\n",
    "        mask_ET[mask_ET == 2] = 0\n",
    "        mask_ET[mask_ET == 3] = 1\n",
    "\n",
    "        mask = np.stack([mask_WT, mask_TC, mask_ET])\n",
    "        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def crop_3d_array(self, arr, crop_shape):\n",
    "        \"\"\"\n",
    "        Crop a 3D array to the specified shape.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : numpy.ndarray\n",
    "            The 3D input array to be cropped.\n",
    "        crop_shape : tuple\n",
    "            The shape of the cropped array. Must be a 3-element tuple (depth, height, width).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The cropped array.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(crop_shape) == 3, \"crop_shape must be a 3-element tuple\"\n",
    "        assert crop_shape[0] <= arr.shape[0], \"depth of crop_shape must be <= depth of arr\"\n",
    "        assert crop_shape[1] <= arr.shape[1], \"height of crop_shape must be <= height of arr\"\n",
    "        assert crop_shape[2] <= arr.shape[2], \"width of crop_shape must be <= width of arr\"\n",
    "\n",
    "        depth_diff = arr.shape[0] - crop_shape[0]\n",
    "        height_diff = arr.shape[1] - crop_shape[1]\n",
    "        width_diff = arr.shape[2] - crop_shape[2]\n",
    "\n",
    "        if depth_diff % 2 == 0:\n",
    "            depth_crop_start = depth_diff // 2\n",
    "            depth_crop_end = arr.shape[0] - (depth_diff // 2)\n",
    "        else:\n",
    "            depth_crop_start = depth_diff // 2\n",
    "            depth_crop_end = arr.shape[0] - (depth_diff // 2) - 1\n",
    "\n",
    "        if height_diff % 2 == 0:\n",
    "            height_crop_start = height_diff // 2\n",
    "            height_crop_end = arr.shape[1] - (height_diff // 2)\n",
    "        else:\n",
    "            height_crop_start = height_diff // 2\n",
    "            height_crop_end = arr.shape[1] - (height_diff // 2) - 1\n",
    "\n",
    "        if width_diff % 2 == 0:\n",
    "            width_crop_start = width_diff // 2\n",
    "            width_crop_end = arr.shape[2] - (width_diff // 2)\n",
    "        else:\n",
    "            width_crop_start = width_diff // 2\n",
    "            width_crop_end = arr.shape[2] - (width_diff // 2) - 1\n",
    "\n",
    "        cropped_arr = arr[depth_crop_start:depth_crop_end,\n",
    "                          height_crop_start:height_crop_end, width_crop_start:width_crop_end]\n",
    "\n",
    "        return cropped_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "def save_prediction(prediction, name, path='predictions/'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    prediction_nii = nib.Nifti1Image(prediction, np.eye(4))\n",
    "    nib.save(prediction_nii, os.path.join(path, f'{name}.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalSingle(target=\"BraTS20_Training_004\", treshold=0.5):\n",
    "    model_name = \"3DOnet_SingleConv_Kernel1_GELU\"\n",
    "    model = ONet3d_v3_GELU(in_channels=4, n_classes=3, n_channels=32).to('cuda')\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    checkpoint_path = \"Logs/3DUnet_your_modifications/your_best_model_20230719-155138.pth\"\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    _, _, test_dataloader = get_dataloaders(\n",
    "      dataset=BratsDataset, path_to_csv=\"./data.csv\", val_fold=0, test_fold=1, batch_size=1, do_resizing=True)\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        name, imgs = data['Id'][0], data['image']\n",
    "        with torch.no_grad():\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            predictions = (probs >= treshold).float()\n",
    "            predictions = predictions.cpu()\n",
    "            for j, pred in enumerate(predictions):\n",
    "                save_prediction(pred.numpy(), name[j])\n",
    "    print(\"Predictions saved as .nii.gz files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Predictions saved as .nii.gz files.\n"
     ]
    }
   ],
   "source": [
    "evalSingle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-2022.10]",
   "language": "python",
   "name": "conda-env-anaconda-2022.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "31158fc4cd4fc4e451ce754c1c3346c52f344d76874f4f3f1f5f3d818ea0c88e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
